---
phase: 14-scheduled-automation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/pipeline/pipeline/lockfile.py
  - apps/pipeline/pipeline/logging_config.py
  - apps/pipeline/main.py
  - apps/pipeline/tests/pipeline/test_lockfile.py
autonomous: true
requirements: [SCHED-02, SCHED-03]

must_haves:
  truths:
    - "Pipeline output is captured to a rotating log file at logs/pipeline.log"
    - "If a pipeline run is already in progress, a second invocation exits cleanly with a clear message"
    - "Lock file is automatically released when the pipeline finishes, crashes, or is killed"
    - "Log files rotate when they exceed a size threshold, keeping the last N files"
  artifacts:
    - path: "apps/pipeline/pipeline/lockfile.py"
      provides: "File-based lock using fcntl.flock for concurrency protection"
      exports: ["PipelineLock"]
    - path: "apps/pipeline/pipeline/logging_config.py"
      provides: "Rotating file handler + console handler logging configuration"
      exports: ["setup_logging"]
    - path: "apps/pipeline/main.py"
      provides: "Lock acquisition at startup + logging init before any pipeline work"
    - path: "apps/pipeline/tests/pipeline/test_lockfile.py"
      provides: "Tests for lock acquisition, contention, and release"
  key_links:
    - from: "apps/pipeline/main.py"
      to: "apps/pipeline/pipeline/lockfile.py"
      via: "PipelineLock context manager wrapping all pipeline execution"
      pattern: "with PipelineLock"
    - from: "apps/pipeline/main.py"
      to: "apps/pipeline/pipeline/logging_config.py"
      via: "setup_logging() called before argument dispatch"
      pattern: "setup_logging"
---

<objective>
Add concurrency protection and rotating log file support to the pipeline.

Purpose: The pipeline must be safe to invoke from a scheduled job without risk of overlapping runs corrupting data, and all output must be captured to inspectable log files for debugging unattended runs.

Output: lockfile.py module, logging_config.py module, updated main.py, tests
</objective>

<execution_context>
@/Users/kyle/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kyle/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@apps/pipeline/main.py
@apps/pipeline/pipeline/paths.py
@apps/pipeline/pipeline/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create lockfile module and rotating log configuration</name>
  <files>
    apps/pipeline/pipeline/lockfile.py
    apps/pipeline/pipeline/logging_config.py
    apps/pipeline/tests/pipeline/test_lockfile.py
  </files>
  <action>
Create two new modules in the pipeline package:

**lockfile.py** -- PipelineLock class using fcntl.flock():
- Lock file path: use `paths.LOGS_DIR / "pipeline.lock"` (ensure LOGS_DIR exists via os.makedirs)
- Implement as a context manager (`__enter__` / `__exit__`):
  - `__enter__`: Open the lock file, attempt `fcntl.flock(fd, LOCK_EX | LOCK_NB)`. On `BlockingIOError` (or `OSError` with errno EWOULDBLOCK), print a clear message like `"[!] Another pipeline run is in progress. Exiting."` and call `sys.exit(0)` (clean exit, not error). On success, write PID to lock file for debugging.
  - `__exit__`: `fcntl.flock(fd, LOCK_UN)`, close file. Do NOT delete the lock file (it's harmless and avoids race conditions).
- The lock is automatically released if the process crashes or is killed (OS releases flock on fd close).

**logging_config.py** -- setup_logging() function:
- Configure Python's root logger with two handlers:
  1. `RotatingFileHandler` targeting `paths.LOGS_DIR / "pipeline.log"`, maxBytes=5MB, backupCount=5 (keeps pipeline.log, pipeline.log.1 through pipeline.log.5)
  2. `StreamHandler` to stderr (so console output continues to work as before)
- Both handlers use format: `"%(asctime)s [%(levelname)s] %(message)s"` with datefmt `"%Y-%m-%d %H:%M:%S"`
- Set root logger level to INFO
- Redirect stdout to a custom class that writes to both the log file and original stdout, so that existing `print()` statements throughout the pipeline are captured in logs without modifying every file. Use a `TeeStream` class that wraps sys.stdout and also writes to the file handler's stream.
- Ensure LOGS_DIR directory exists (os.makedirs with exist_ok=True)

**test_lockfile.py** -- Tests:
- Test that PipelineLock acquires lock successfully (enters and exits without error)
- Test that a second PipelineLock attempt while first is held calls sys.exit(0) (use mock or subprocess)
- Test that lock is released after context manager exits (second attempt succeeds after first completes)
- Use tmp_path fixture for lock file path (parameterize PipelineLock to accept custom path for testing)
  </action>
  <verify>
Run: `cd /Users/kyle/development/viewroyal/apps/pipeline && uv run pytest tests/pipeline/test_lockfile.py -v`
All tests pass. Verify lockfile.py and logging_config.py exist and contain the expected classes/functions.
  </verify>
  <done>
PipelineLock context manager correctly acquires/releases flock, exits cleanly on contention. setup_logging() configures rotating file + console handlers with stdout tee. Tests prove lock contention behavior.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire lockfile and logging into main.py entrypoint</name>
  <files>
    apps/pipeline/main.py
  </files>
  <action>
Modify main.py to integrate lock and logging:

1. After `args = parser.parse_args()` and before any pipeline work (before municipality loading), add:
   ```python
   from pipeline.logging_config import setup_logging
   setup_logging()
   ```

2. Wrap the entire pipeline dispatch block (everything from `municipality = None` through the end of the if/elif chain) inside a `PipelineLock` context manager:
   ```python
   from pipeline.lockfile import PipelineLock
   with PipelineLock():
       # ... existing dispatch code
   ```

3. This means: if lock cannot be acquired, the script exits before doing ANY work. If it can, all pipeline operations run under the lock, and the lock is released when done (or on crash).

4. Do NOT change any existing print() statements -- the TeeStream in logging_config handles capturing them.

5. Add a startup log line inside the lock: `print(f"[*] Pipeline started at {datetime.now().isoformat()}")` and a completion line at the end: `print(f"[*] Pipeline finished at {datetime.now().isoformat()}")`. Import datetime at top of file.
  </action>
  <verify>
Run: `cd /Users/kyle/development/viewroyal/apps/pipeline && uv run python main.py --help` (should work, no import errors).
Run: `cd /Users/kyle/development/viewroyal/apps/pipeline && uv run pytest tests/ -v --timeout=30` (existing tests still pass).
Check that `logs/` directory is created and `pipeline.log` would be the target.
  </verify>
  <done>
main.py acquires lock before any pipeline work, sets up logging with rotation, and releases lock on exit. Existing tests unbroken. Running `--help` confirms no import errors.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/pipeline/test_lockfile.py -v` -- all lock tests pass
2. `uv run pytest tests/ -v` -- all existing tests still pass
3. `uv run python main.py --help` -- no import errors, help text displays
4. Manual: run `uv run python main.py --check-updates` and verify logs/pipeline.log is created with output
</verification>

<success_criteria>
- lockfile.py exists with PipelineLock context manager using fcntl.flock
- logging_config.py exists with setup_logging() configuring RotatingFileHandler (5MB, 5 backups)
- main.py wraps all pipeline work in PipelineLock and calls setup_logging()
- Lock contention test proves second invocation exits cleanly
- All existing pipeline tests pass without modification
</success_criteria>

<output>
After completion, create `.planning/phases/14-scheduled-automation/14-01-SUMMARY.md`
</output>
