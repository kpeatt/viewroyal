---
phase: 07.1-upgrade-document-extraction
plan: 02
type: execute
wave: 2
depends_on: ["07.1-01"]
files_modified:
  - apps/pipeline/pipeline/ingestion/image_extractor.py
  - apps/pipeline/pipeline/ingestion/document_extractor.py
  - apps/pipeline/pipeline/ingestion/ingester.py
  - apps/pipeline/pipeline/orchestrator.py
  - apps/pipeline/pyproject.toml
autonomous: true
requirements: [DOC-01, DOC-04]
user_setup:
  - service: cloudflare-r2
    why: "Store extracted document images for edge serving"
    env_vars:
      - name: R2_ACCESS_KEY_ID
        source: "Cloudflare Dashboard -> R2 -> Manage R2 API Tokens -> Create API token (Object Read & Write for specific bucket)"
      - name: R2_SECRET_ACCESS_KEY
        source: "Same R2 API token creation page"
      - name: R2_ENDPOINT_URL
        source: "Cloudflare Dashboard -> R2 -> bucket details -> S3 API endpoint (format: https://<account_id>.r2.cloudflarestorage.com)"
      - name: R2_BUCKET_NAME
        source: "Create bucket named 'viewroyal-document-images' in R2 dashboard"
    dashboard_config:
      - task: "Create R2 bucket named 'viewroyal-document-images'"
        location: "Cloudflare Dashboard -> R2 Object Storage -> Create bucket"
      - task: "Create R2 API token with Object Read & Write"
        location: "Cloudflare Dashboard -> R2 -> Manage R2 API Tokens"

must_haves:
  truths:
    - "PyMuPDF extracts meaningful images from PDFs, skipping logos and decorative graphics"
    - "Images upload to Cloudflare R2 via S3-compatible API"
    - "Document extractor orchestrates Gemini boundary detection, content extraction, image extraction, and section creation"
    - "Ingester calls new extractor instead of old chunker for agenda PDFs"
    - "Orchestrator backfill method uses new extractor pipeline"
    - "Docling dependency removed from pyproject.toml"
  artifacts:
    - path: "apps/pipeline/pipeline/ingestion/image_extractor.py"
      provides: "PyMuPDF image extraction with R2 upload"
      min_lines: 80
    - path: "apps/pipeline/pipeline/ingestion/document_extractor.py"
      provides: "Orchestrator connecting Gemini + images + section creation"
      min_lines: 150
  key_links:
    - from: "apps/pipeline/pipeline/ingestion/document_extractor.py"
      to: "apps/pipeline/pipeline/ingestion/gemini_extractor.py"
      via: "import and function calls"
      pattern: "from pipeline\\.ingestion\\.gemini_extractor import"
    - from: "apps/pipeline/pipeline/ingestion/document_extractor.py"
      to: "apps/pipeline/pipeline/ingestion/image_extractor.py"
      via: "import and function calls"
      pattern: "from pipeline\\.ingestion\\.image_extractor import"
    - from: "apps/pipeline/pipeline/ingestion/ingester.py"
      to: "apps/pipeline/pipeline/ingestion/document_extractor.py"
      via: "_ingest_document_sections calls extract_documents"
      pattern: "from pipeline\\.ingestion\\.document_extractor import"
---

<objective>
Build the image extraction module, document extraction orchestrator, and integrate the new pipeline into ingester.py and orchestrator.py. Remove Docling dependency.

Purpose: Connects the Gemini extractor (Plan 01) into the full pipeline flow, adds image extraction capability, and replaces the old PyMuPDF font-analysis chunker as the primary extraction path.
Output: Complete extraction pipeline that processes agenda PDFs through Gemini boundary detection, Gemini content extraction, PyMuPDF image extraction, and inserts results into extracted_documents + document_sections + document_images tables.
</objective>

<execution_context>
@/Users/kyle/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kyle/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-CONTEXT.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-01-SUMMARY.md
@apps/pipeline/pipeline/ingestion/gemini_extractor.py
@apps/pipeline/pipeline/ingestion/ingester.py
@apps/pipeline/pipeline/orchestrator.py
@apps/pipeline/pipeline/ingestion/document_chunker.py
@apps/pipeline/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create image extractor and document extraction orchestrator</name>
  <files>
    apps/pipeline/pipeline/ingestion/image_extractor.py
    apps/pipeline/pipeline/ingestion/document_extractor.py
  </files>
  <action>
**1. Create `image_extractor.py`** — PyMuPDF image extraction with R2 upload:

```python
"""
PyMuPDF image extraction with Cloudflare R2 upload.

Extracts meaningful images (maps, charts, diagrams, renderings) from PDFs,
filtering out decorative graphics, logos, and signatures based on dimensions.
Uploads to R2 for edge serving.
"""
```

**Constants:**
- `MIN_WIDTH = 100` — skip images narrower than 100px
- `MIN_HEIGHT = 100` — skip images shorter than 100px
- `MIN_AREA = 20000` — skip images smaller than 20K sq pixels
- `MAX_ASPECT_RATIO = 5.0` — skip very wide/short images (horizontal rules)

**Functions:**

1. `extract_images(pdf_path: str, page_start: int, page_end: int) -> list[dict]`
   - Open PDF with PyMuPDF (`fitz.open`)
   - Iterate pages in range (0-indexed internally, 1-indexed parameters)
   - For each page, get images via `page.get_images(full=True)`
   - For each image, extract via `doc.extract_image(xref)`
   - Apply dimension filters (MIN_WIDTH, MIN_HEIGHT, MIN_AREA, MAX_ASPECT_RATIO)
   - Deduplicate by xref (same image may appear on multiple pages — keep first occurrence)
   - Return list of dicts: `{xref, page, width, height, format, data (bytes)}`

2. `upload_images_to_r2(images: list[dict], meeting_id: int, extracted_doc_id: int) -> list[dict]`
   - Read R2 credentials from env: `R2_ACCESS_KEY_ID`, `R2_SECRET_ACCESS_KEY`, `R2_ENDPOINT_URL`, `R2_BUCKET_NAME`
   - If any credential is missing, log warning and return empty list (graceful degradation — images are optional)
   - Use `boto3` S3 client with R2 endpoint
   - For each image, upload to key: `documents/{meeting_id}/{extracted_doc_id}/{xref}.{format}`
   - Set content-type based on format (png, jpeg, etc.)
   - Return list of dicts: `{r2_key, page, width, height, format, file_size}`

3. `get_r2_client()` — lazy singleton boto3 S3 client configured for R2

**Error handling:**
- If boto3 is not installed, log warning and return empty lists
- If R2 credentials missing, log once and skip all uploads
- Individual image extraction failures should be caught and logged, not abort the batch

**2. Create `document_extractor.py`** — orchestrates the full extraction pipeline:

```python
"""
Document extraction orchestrator.

Coordinates Gemini boundary detection, Gemini content extraction,
PyMuPDF image extraction, and database insertion for agenda PDFs.

Replaces the old PyMuPDF font-analysis document_chunker.py as the
primary extraction path.
"""
```

**Functions:**

1. `extract_and_store_documents(pdf_path: str, document_id: int, meeting_id: int, supabase, municipality_id: int = 1) -> dict`

   Main entry point. Steps:

   a. Call `gemini_extractor.detect_boundaries(pdf_path)` to get document boundaries
   b. If boundaries empty (Gemini failed), fall back to old chunker: call `document_chunker.chunk_document()` + `link_sections_to_agenda_items()` and insert into document_sections directly. Log the fallback. Return early.
   c. For each boundary document:
      - Look up agenda_item_id: if boundary has `agenda_item` string, query `agenda_items` table for this meeting to find matching item by number/title. Use similar logic to the existing `link_sections_to_agenda_items` number-pattern strategy, but Gemini provides cleaner input.
      - Insert into `extracted_documents` table: document_id, agenda_item_id, title, document_type, page_start, page_end, summary, key_facts, municipality_id
      - Call `gemini_extractor.extract_content(pdf_path, page_start, page_end, title)` to get markdown
      - Split markdown into sections using `_split_markdown_into_sections(markdown)` helper
      - Insert sections into `document_sections`: document_id, extracted_document_id, agenda_item_id (same as parent), section_title, section_text, section_order, page_start, page_end, token_count, municipality_id
      - Call `image_extractor.extract_images(pdf_path, page_start, page_end)` for images
      - If images found, call `image_extractor.upload_images_to_r2(images, meeting_id, extracted_doc_id)`
      - Insert image metadata into `document_images` table
   d. Return stats dict: `{boundaries_found, documents_extracted, sections_created, images_extracted}`

2. `_split_markdown_into_sections(markdown: str, max_chars: int = 8000) -> list[dict]`
   - Split Gemini's markdown output into sections at `## ` heading boundaries
   - If no headings found, use the entire content as a single section with title from the document
   - Sections exceeding `max_chars` get split at paragraph boundaries (double newline) with "Part N of M" suffix on title
   - Each section dict: `{section_title, section_text, section_order (1-indexed), token_count}`
   - Token count estimated as `len(text.split()) * 1.3` (matching existing pattern in document_chunker.py)

3. `_resolve_agenda_item(agenda_item_str: str, meeting_id: int, supabase) -> int | None`
   - Given Gemini's agenda_item string (e.g., "6.1a", "3.a)", "8.1a)"), find the matching agenda_item row
   - Query agenda_items for this meeting_id
   - Normalize both sides: strip parentheses, periods, whitespace
   - Try exact match on item_number field first
   - Try containment match (agenda_item_str in item_number or vice versa)
   - Return agenda_item_id or None if no match

**Key design decisions:**
- Fallback to old chunker if Gemini fails (per Claude's Discretion on fallback strategy)
- Agenda item linking uses Gemini's output directly rather than the old 4-strategy heuristic
- Sections split at markdown heading boundaries, not font-size analysis
- Inline markdown tables in section_text (per locked decision)
  </action>
  <verify>
```bash
cd /Users/kyle/development/viewroyal/apps/pipeline && uv run python -c "
from pipeline.ingestion.image_extractor import extract_images, upload_images_to_r2
from pipeline.ingestion.document_extractor import extract_and_store_documents, _split_markdown_into_sections
# Test section splitting
sections = _split_markdown_into_sections('## Heading 1\nContent here\n\n## Heading 2\nMore content')
assert len(sections) == 2, f'Expected 2 sections, got {len(sections)}'
assert sections[0]['section_title'] == 'Heading 1'
assert sections[1]['section_title'] == 'Heading 2'
print('All imports and basic tests pass')
"
```
  </verify>
  <done>image_extractor.py and document_extractor.py exist, import cleanly, and section splitting produces correct output from markdown headings.</done>
</task>

<task type="auto">
  <name>Task 2: Integrate new extractor into pipeline and remove Docling</name>
  <files>
    apps/pipeline/pipeline/ingestion/ingester.py
    apps/pipeline/pipeline/orchestrator.py
    apps/pipeline/pyproject.toml
  </files>
  <action>
**1. Update `ingester.py` `_ingest_document_sections()` method** (lines ~769-855):

Replace the body of `_ingest_document_sections()` to use the new pipeline:

```python
def _ingest_document_sections(self, meeting_id, folder_path):
    """Extract documents from PDFs using Gemini + PyMuPDF pipeline.

    For each document with a file_path (agenda PDFs), runs:
    1. Gemini boundary detection (document boundaries, types, agenda links)
    2. Gemini content extraction (markdown per document)
    3. PyMuPDF image extraction + R2 upload
    4. Section splitting and insertion

    Falls back to PyMuPDF font-analysis chunker if Gemini fails.
    Skips documents that already have extracted_documents (idempotency).
    """
```

Key changes:
- Import `extract_and_store_documents` from `document_extractor`
- Fetch documents for meeting (same as before)
- For each document, check idempotency: query `extracted_documents` for this `document_id` — skip if rows exist
- Resolve full PDF path (same as before)
- Call `extract_and_store_documents(pdf_path, doc_id, meeting_id, self.supabase, self.municipality_id)`
- Log the returned stats

**2. Update `orchestrator.py` `backfill_document_sections()` method** (lines ~435-558):

Replace the body to use the new extractor pipeline:

```python
def backfill_document_sections(self, force=False):
    """Backfill extracted documents and sections for all existing documents.

    Uses Gemini 2.5 Flash for boundary detection + content extraction.
    Falls back to PyMuPDF chunker if Gemini fails.

    Two-pass approach:
    Pass 1 (this method): Extract and create sections for all documents
    Pass 2 (called separately): _embed_new_content() generates embeddings
    """
```

Key changes:
- Import `extract_and_store_documents` from `document_extractor`
- If `force=True`, delete ALL rows from `extracted_documents`, `document_images`, and `document_sections` at the start (CASCADE will handle child rows from extracted_documents)
- For each document: check idempotency on `extracted_documents` table (not `document_sections`)
- Call `extract_and_store_documents()` instead of `chunk_document()` + `link_sections_to_agenda_items()`
- Keep the same error handling pattern (log and continue)
- Add a small `time.sleep(1)` between documents to respect Gemini rate limits during bulk processing

**3. Remove Docling from `pyproject.toml`**:
- Remove `"docling>=2.73.1"` from the dependencies list
- This is a locked decision from CONTEXT.md: "Docling has been dropped"

**4. Add `boto3` to `pyproject.toml`**:
- Add `"boto3>=1.35.0"` to the dependencies list
- Required for R2 S3-compatible API uploads from image_extractor.py

Do NOT modify `document_chunker.py` — it is kept as a fallback module.
  </action>
  <verify>
```bash
cd /Users/kyle/development/viewroyal/apps/pipeline && uv run python -c "
from pipeline.ingestion.ingester import Ingester
from pipeline.orchestrator import Archiver
# Verify the new imports work
from pipeline.ingestion.document_extractor import extract_and_store_documents
print('Pipeline integration imports OK')
" && grep -c 'docling' pyproject.toml | xargs -I {} bash -c 'if [ {} -eq 0 ]; then echo "Docling removed: OK"; else echo "Docling still present: FAIL"; fi' && grep 'boto3' pyproject.toml && echo "boto3 added: OK"
```
  </verify>
  <done>ingester.py calls new document_extractor instead of old chunker. orchestrator.py backfill uses new pipeline with delete-and-replace for force mode. Docling removed from pyproject.toml. boto3 added to pyproject.toml.</done>
</task>

</tasks>

<verification>
- image_extractor.py exists with extract_images() and upload_images_to_r2() functions
- document_extractor.py exists with extract_and_store_documents() as main entry point
- ingester.py _ingest_document_sections() calls document_extractor not document_chunker
- orchestrator.py backfill_document_sections() uses new pipeline
- pyproject.toml has boto3, does NOT have docling
- document_chunker.py still exists as fallback (not deleted)
- Running `uv run python -c "from pipeline.ingestion.document_extractor import extract_and_store_documents"` succeeds
</verification>

<success_criteria>
Full extraction pipeline is integrated: agenda PDFs go through Gemini boundary detection, Gemini content extraction, PyMuPDF image extraction, and results are stored in extracted_documents, document_sections, and document_images tables. Old chunker remains as automatic fallback. Docling dependency removed.
</success_criteria>

<output>
After completion, create `.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-02-SUMMARY.md`
</output>
