---
phase: 07.1-upgrade-document-extraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/pipeline/pipeline/ingestion/gemini_extractor.py
autonomous: true
requirements: [DOC-01, DOC-02, DOC-03, DOC-04]

must_haves:
  truths:
    - "extracted_documents table exists with document_type, page_start, page_end, summary, key_facts columns"
    - "document_images table exists with r2_key, page, description, image_type, width, height columns"
    - "document_sections has an extracted_document_id FK column"
    - "Gemini extractor can detect document boundaries in an agenda PDF and return structured JSON"
    - "Gemini extractor can extract clean markdown content for a document's page range"
    - "PDFs under 20MB use inline bytes, 20-50MB use File API"
  artifacts:
    - path: "apps/pipeline/pipeline/ingestion/gemini_extractor.py"
      provides: "Gemini 2.5 Flash two-pass extraction (boundaries + content)"
      min_lines: 150
  key_links:
    - from: "apps/pipeline/pipeline/ingestion/gemini_extractor.py"
      to: "google.genai"
      via: "Client API calls"
      pattern: "client\\.models\\.generate_content"
---

<objective>
Create the database schema for extracted documents and document images, plus the Gemini 2.5 Flash two-pass extractor module that handles boundary detection and content extraction from agenda PDFs.

Purpose: Establishes the data model for the new extraction pipeline and builds the core AI extraction module that replaces PyMuPDF font-analysis heading detection.
Output: `extracted_documents` and `document_images` tables in Supabase, `gemini_extractor.py` module with boundary detection and content extraction functions.
</objective>

<execution_context>
@/Users/kyle/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kyle/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-CONTEXT.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-RESEARCH.md
@.planning/phases/07-document-intelligence/07-01-SUMMARY.md
@apps/pipeline/pipeline/ingestion/ingester.py
@apps/pipeline/pipeline/ingestion/embed.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create extracted_documents and document_images tables + update document_sections</name>
  <files>supabase migration (applied via MCP)</files>
  <action>
Create a Supabase migration with three schema changes:

1. **Create `extracted_documents` table** — intermediate layer between `documents` (source PDF files) and `document_sections`:
   ```sql
   CREATE TABLE extracted_documents (
     id bigint generated by default as identity primary key,
     document_id bigint REFERENCES documents(id) ON DELETE CASCADE NOT NULL,
     agenda_item_id bigint REFERENCES agenda_items(id) ON DELETE SET NULL,
     title text NOT NULL,
     document_type text NOT NULL DEFAULT 'other',
     page_start integer,
     page_end integer,
     summary text,
     key_facts jsonb,
     municipality_id bigint REFERENCES municipalities(id) DEFAULT 1,
     created_at timestamptz DEFAULT timezone('utc'::text, now()) NOT NULL
   );
   ```
   - Add index on `document_id` for fast lookups
   - Add index on `agenda_item_id` for linking queries
   - Add index on `document_type` for type filtering
   - Enable RLS with a permissive SELECT policy for authenticated and anon users (same pattern as document_sections)
   - `document_type` values: agenda, minutes, staff_report, delegation, correspondence, appendix, bylaw, presentation, form, other

2. **Create `document_images` table** for R2 image metadata:
   ```sql
   CREATE TABLE document_images (
     id bigint generated by default as identity primary key,
     extracted_document_id bigint REFERENCES extracted_documents(id) ON DELETE CASCADE NOT NULL,
     r2_key text NOT NULL,
     page integer NOT NULL,
     description text,
     image_type text DEFAULT 'other',
     width integer,
     height integer,
     format text,
     file_size integer,
     municipality_id bigint REFERENCES municipalities(id) DEFAULT 1,
     created_at timestamptz DEFAULT timezone('utc'::text, now()) NOT NULL
   );
   ```
   - Add index on `extracted_document_id`
   - Enable RLS with permissive SELECT policy
   - `image_type` values: map, site_plan, chart, diagram, rendering, photo, other

3. **Add `extracted_document_id` column to `document_sections`**:
   ```sql
   ALTER TABLE document_sections
     ADD COLUMN extracted_document_id bigint REFERENCES extracted_documents(id) ON DELETE CASCADE;
   ```
   - Add index on `extracted_document_id`
   - Keep existing `document_id` FK for backward compatibility (existing web queries use it)

Use the Supabase MCP `apply_migration` tool. Name the migration `add_extracted_documents_and_images`.
  </action>
  <verify>
Run SQL queries via MCP to confirm:
- `SELECT column_name FROM information_schema.columns WHERE table_name = 'extracted_documents'` shows expected columns
- `SELECT column_name FROM information_schema.columns WHERE table_name = 'document_images'` shows expected columns
- `SELECT column_name FROM information_schema.columns WHERE table_name = 'document_sections' AND column_name = 'extracted_document_id'` returns a row
  </verify>
  <done>All three tables/columns exist in database with proper indexes and RLS policies.</done>
</task>

<task type="auto">
  <name>Task 2: Create Gemini 2.5 Flash two-pass extractor module</name>
  <files>apps/pipeline/pipeline/ingestion/gemini_extractor.py</files>
  <action>
Create `gemini_extractor.py` with the following functions, based on the proven test script at `/tmp/test_gemini25_flash.py`:

**Module structure:**

```python
"""
Gemini 2.5 Flash two-pass document extraction.

Pass 1 (boundary detection): Send full agenda PDF, get structured JSON with
document boundaries, types, agenda item links, summaries, and key facts.

Pass 2 (content extraction): For each document boundary, extract clean
structured markdown content for those pages.
"""
```

**Configuration:**
- `GEMINI_MODEL` — default `"gemini-2.5-flash"`, read from env var `GEMINI_MODEL` for easy swapping
- `MAX_INLINE_MB = 20` — PDFs under this use inline bytes
- `MAX_FILE_API_MB = 50` — PDFs between 20-50MB use File API upload
- `MAX_OUTPUT_TOKENS = 65536` — Gemini 2.5 Flash supports 64K output

**Functions:**

1. `get_gemini_client() -> genai.Client` — lazy singleton, reads `GEMINI_API_KEY` from env

2. `detect_boundaries(pdf_path: str) -> list[dict]` — Pass 1
   - Read PDF file, check size
   - If < 20MB: send inline via `types.Part.from_bytes(data=pdf_bytes, mime_type="application/pdf")`
   - If 20-50MB: upload via `client.files.upload(file=pdf_path)`, send file reference
   - If > 50MB: split PDF with PyMuPDF into chunks (see `_split_large_pdf()`), run boundary detection on each chunk, merge results adjusting page numbers
   - Use the boundary detection prompt from `/tmp/test_gemini25_flash.py` (the `BOUNDARY_PROMPT` constant — copy it exactly, it's proven working)
   - Parse JSON from response (handle ```json fencing)
   - Return list of dicts with keys: title, page_start, page_end, type, agenda_item, summary, key_facts
   - Log token usage and cost estimate
   - Handle `finish_reason == MAX_TOKENS` by logging a warning (boundary JSON was truncated)

3. `extract_content(pdf_path: str, page_start: int, page_end: int, doc_title: str) -> str` — Pass 2
   - Send the full PDF with a page-specific prompt asking for content extraction of pages page_start to page_end
   - Prompt template (proven in test script):
     ```
     Extract the full content of the document on pages {page_start} to {page_end} from this PDF.
     Output clean, well-structured markdown:
     - Use ## for main headings, ### for sub-headings
     - Render tables as markdown tables
     - Preserve numbered/bulleted lists
     - Include staff recommendations verbatim
     - Skip headers/footers, page numbers, and watermarks
     - Skip any images but note where they appear as [Image: brief description]
     - Preserve all dollar amounts, dates, addresses, and bylaw numbers exactly
     Return ONLY the markdown content, no commentary.
     ```
   - Return the markdown string
   - Log token usage

4. `_split_large_pdf(pdf_path: str, max_pages: int = 500) -> list[tuple[str, int]]` — helper
   - Use PyMuPDF to split PDF into chunks of max_pages
   - Return list of (temp_file_path, page_offset) tuples
   - Include the first 4 pages (agenda/TOC) in every chunk so Gemini can always reference agenda items
   - Clean up temp files after use (caller responsibility via context manager or explicit cleanup)

5. `_parse_json_response(text: str) -> list[dict]` — helper
   - Strip ```json fencing if present
   - Parse JSON
   - Validate each entry has required fields (title, page_start, page_end, type)
   - Return parsed list or empty list on failure (log error)

**Error handling:**
- Wrap all Gemini API calls in try/except, log errors, return empty list / empty string on failure
- Retry once on transient errors (rate limit, server error) with 5-second sleep
- Use `logging` module consistently (logger = logging.getLogger(__name__))

**Key implementation notes:**
- The test script sends the full PDF for both boundary detection AND content extraction (not page-range extracts). This is by design — Gemini handles multi-hundred-page PDFs natively.
- Model name must be configurable via env var for easy migration from 2.5 Flash to future versions.
- Do NOT use Docling anywhere — it has been dropped.
  </action>
  <verify>
```bash
cd /Users/kyle/development/viewroyal/apps/pipeline && uv run python -c "
from pipeline.ingestion.gemini_extractor import detect_boundaries, extract_content, get_gemini_client, GEMINI_MODEL
print(f'Model: {GEMINI_MODEL}')
print('Module imports OK')
print(f'Client type: {type(get_gemini_client()).__name__}')
"
```
Module imports successfully, model name is configurable.
  </verify>
  <done>gemini_extractor.py exists with detect_boundaries() and extract_content() functions. Module imports cleanly. Model name defaults to gemini-2.5-flash and is configurable via GEMINI_MODEL env var.</done>
</task>

</tasks>

<verification>
- extracted_documents table exists with all columns (document_id, title, document_type, page_start, page_end, summary, key_facts, agenda_item_id)
- document_images table exists with all columns (extracted_document_id, r2_key, page, description, image_type, width, height)
- document_sections.extracted_document_id column exists
- gemini_extractor.py imports cleanly
- detect_boundaries() and extract_content() functions exist and are callable
- Model name configurable via env var
</verification>

<success_criteria>
Schema is ready for extraction pipeline. Gemini extractor module can detect document boundaries and extract content from PDFs using Gemini 2.5 Flash API. The module handles inline bytes, File API, and large PDF splitting.
</success_criteria>

<output>
After completion, create `.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-01-SUMMARY.md`
</output>
