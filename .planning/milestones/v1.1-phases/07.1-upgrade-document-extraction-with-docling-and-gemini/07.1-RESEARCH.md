# Phase 07.1: Upgrade Document Extraction with Docling and Gemini - Research

**Researched:** 2026-02-17
**Domain:** PDF document extraction, AI-powered document analysis, structured content extraction
**Confidence:** HIGH

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions
- Gemini's role is **metadata only**: document boundaries, agenda item linking, document type classification, and key_facts
- Gemini does NOT extract content -- that's Docling's job
- Process the full agenda package PDF through Gemini to identify individual documents within it (staff reports, delegations, correspondence, etc.)
- Return structured JSON with: title, page_start, page_end, type (agenda/minutes/staff_report/delegation/correspondence/appendix/table/other), agenda_item mapping, summary, key_facts
- Must handle Gemini's document size limits for large PDFs (some are 200-700+ pages)
- Test scripts at `/tmp/gemini_test.py` and `/tmp/gemini_test_structured.py` -- proven working with Gemini 2.0 Flash
- Docling extracts structured markdown content within document boundaries identified by Gemini
- Replaces PyMuPDF dict-mode font analysis for heading detection and section splitting
- Tables rendered as markdown tables inline in section content
- Test script at `/tmp/test_docling.py` -- proven working, 30 pages in 20.9s, clean structural hierarchy
- Schema may evolve to better reflect the natural hierarchy: agenda package -> individual documents -> sections
- PyMuPDF extracts all meaningful images: maps, site plans, charts, diagrams, architectural renderings
- Skip decorative graphics, logos, and signatures
- Store extracted images on **Cloudflare R2** for edge serving
- **Store only for now** -- do NOT update the document viewer UI to display images (that's a future phase)
- Track image metadata: page, description (from Gemini), type classification, dimensions
- **Delete and replace** all existing document_sections data
- **Update the pipeline code** for new meetings AND run backfill for all existing meetings
- Process ALL 714 meetings with agenda PDFs from local archive (`/viewroyal_archive/`)
- Archive structure: `{meeting_type}/{year}/{month}/{meeting_name}/Agenda/*.pdf`
- **Resumable processing** -- track which meetings have been processed so backfill can be interrupted and resumed
- Estimated Gemini cost: ~$6-10 for full backfill of ~65K pages

### Claude's Discretion
- PDF splitting strategy for Gemini's size limit (chunking vs TOC-guided vs other)
- Agenda item linking approach (two-pass with TOC, or include agenda pages in every chunk)
- Database schema design -- whether to add a document-level table between `documents` (source files) and `document_sections`
- PyMuPDF font-analysis fallback strategy (keep as fallback or fully replace)
- Table storage approach (inline markdown vs separate structured storage)
- Which individual attachments beyond the main agenda PDF to process (if any)

### Deferred Ideas (OUT OF SCOPE)
- Display extracted images in the document viewer UI -- future phase
- Positional/sequential matching for agenda item linking (Strategy 4 from Phase 7) -- evaluate if Gemini makes this unnecessary
- Full-text search improvements using better-structured sections -- Phase 8 (Unified Search)
</user_constraints>

<phase_requirements>
## Phase Requirements

| ID | Description | Research Support |
|----|-------------|-----------------|
| DOC-01 | Pipeline chunks PDF documents into sections using heading-based parsing with fixed-size fallback | Docling replaces PyMuPDF font-analysis; provides native heading hierarchy detection with ML-based layout analysis; fixed-size fallback remains for scanned PDFs |
| DOC-02 | Document sections stored in `document_sections` table with per-section halfvec(384) embeddings | Existing schema is compatible; new pipeline outputs same fields; embedding generation via existing `embed.py` unchanged |
| DOC-03 | Document sections have tsvector full-text search indexes | Existing `text_search` generated column on `document_sections` table is already in production; no schema change needed |
| DOC-04 | Pipeline links document sections to corresponding agenda items via title matching | Gemini provides direct agenda_item mapping per document boundary; replaces heuristic multi-strategy linking with AI-powered linking |
| DOC-05 | Existing documents backfilled into sections with embeddings | Full backfill of all 711+ meetings with agenda PDFs from local archive; delete-and-replace strategy with resumable tracking |
</phase_requirements>

## Summary

This phase replaces the current PyMuPDF font-analysis document extraction pipeline with a three-tool approach: Gemini 2.0 Flash for document boundary detection and metadata, Docling for structured content extraction within those boundaries, and PyMuPDF for meaningful image extraction. The existing codebase has proven test scripts for both Gemini and Docling, and both libraries are already dependencies in `pyproject.toml`.

The current pipeline (`document_chunker.py`, 662 lines) uses PyMuPDF dict-mode font analysis to detect headings via font size thresholds (body_size * 1.2), then splits documents at heading boundaries. This approach struggles with: inconsistent font usage across PDFs, inability to identify document boundaries within agenda packages, and no semantic understanding of document types. The new approach leverages Gemini's multimodal understanding to identify document boundaries and metadata, then uses Docling's ML-based layout analysis for structured content extraction.

**Primary recommendation:** Implement a three-stage pipeline: (1) Gemini boundary detection per agenda package PDF, (2) Docling extraction within boundaries using page_range, (3) PyMuPDF image extraction with dimension-based filtering. Use the existing `document_sections` table with an added `document_type` column and optional new `extracted_documents` intermediate table.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| google-genai | >=1.59.0 | Gemini 2.0 Flash API for PDF metadata extraction | Already in pyproject.toml; proven working in test scripts |
| docling | >=2.73.1 | ML-based PDF layout analysis and content extraction | Already in pyproject.toml; proven working in test scripts; handles tables, headings, hierarchy |
| PyMuPDF (fitz) | >=1.26.7 | Image extraction from PDFs | Already in pyproject.toml; proven for image extraction with dimension filtering |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| tqdm | >=4.67.1 | Progress bars for backfill processing | Already in pyproject.toml; use for 711-meeting backfill progress |
| supabase | >=2.27.2 | Database operations | Already in pyproject.toml; existing pattern for document_sections |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Docling | marker-pdf (already in deps) | marker-pdf already used for OCR fallback; Docling has better table extraction and heading hierarchy |
| Gemini 2.0 Flash | Gemini 2.5 Flash | 2.5 Flash has "thinking" mode and may be better but costs more; 2.0 Flash proven working in tests |
| PyMuPDF images | Docling pictures | Docling `generate_picture_images=True` extracts images but PyMuPDF gives more control over filtering |

**No new installations needed** -- all libraries already in `apps/pipeline/pyproject.toml`.

## Architecture Patterns

### Current Pipeline Flow (Being Replaced)
```
PDF -> PyMuPDF dict-mode font analysis -> heading detection (font_size > body_size * 1.2)
    -> split at headings -> size cap (8000 chars) -> link to agenda items (4 strategies)
    -> insert document_sections
```

### New Pipeline Flow
```
Agenda PDF -> Gemini 2.0 Flash -> document boundaries + metadata JSON
                                    |
    For each document boundary:     |
        Docling (page_range) -------+-> structured markdown sections
        PyMuPDF (page_range) -------+-> extracted images -> R2 upload
                                    |
    -> insert extracted_documents + document_sections
```

### Recommended Module Structure
```
apps/pipeline/pipeline/
├── ingestion/
│   ├── document_chunker.py       # EXISTING: keep as fallback, refactor
│   ├── document_extractor.py     # NEW: orchestrates Gemini + Docling + PyMuPDF
│   ├── gemini_boundary.py        # NEW: Gemini document boundary detection
│   ├── docling_extractor.py      # NEW: Docling content extraction
│   ├── image_extractor.py        # NEW: PyMuPDF image extraction + R2 upload
│   ├── ingester.py               # MODIFY: call new extractor instead of chunker
│   └── embed.py                  # UNCHANGED: already handles document_sections
```

### Pattern 1: Gemini Boundary Detection
**What:** Send full agenda PDF to Gemini, get back structured JSON with document boundaries
**When to use:** For every agenda package PDF during ingestion

```python
# Source: Proven in /tmp/gemini_test.py, /tmp/gemini_test_structured.py
from google import genai
from google.genai import types

client = genai.Client(api_key=api_key)

# For PDFs under 50MB (most agenda packages):
with open(pdf_path, "rb") as f:
    pdf_bytes = f.read()

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=[
        types.Part.from_bytes(data=pdf_bytes, mime_type="application/pdf"),
        BOUNDARY_DETECTION_PROMPT
    ],
)

# For PDFs over 50MB: use File API
file = client.files.upload(file=pdf_path, config={"mime_type": "application/pdf"})
response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=[file, BOUNDARY_DETECTION_PROMPT],
)
```

### Pattern 2: Docling Page-Range Extraction
**What:** Extract structured content from specific page ranges identified by Gemini
**When to use:** For each document boundary detected by Gemini

```python
# Source: Proven in /tmp/test_docling.py; page_range from docling docs
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions

pipeline_options = PdfPipelineOptions()
pipeline_options.do_ocr = False  # Skip OCR for digital-native PDFs
pipeline_options.generate_picture_images = False  # We use PyMuPDF for images

converter = DocumentConverter(
    format_options={
        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    }
)

# Extract specific page range (1-indexed inclusive)
result = converter.convert(
    source=pdf_path,
    page_range=(page_start, page_end)
)

markdown = result.document.export_to_markdown()

# Iterate items for structured access
for item, level in result.document.iterate_items():
    # item has .label, .text, etc.
    pass
```

### Pattern 3: PyMuPDF Image Extraction with Filtering
**What:** Extract meaningful images, skip decorative ones
**When to use:** For each document boundary, extract images from that page range

```python
# Source: PyMuPDF docs (pymupdf.readthedocs.io/en/latest/recipes-images.html)
import fitz

MIN_WIDTH = 100   # Skip images narrower than 100px
MIN_HEIGHT = 100  # Skip images shorter than 100px
MIN_AREA = 20000  # Skip images smaller than 20K sq pixels

doc = fitz.open(pdf_path)
images = []

for page_num in range(page_start - 1, page_end):  # 0-indexed
    page = doc[page_num]
    for img_info in page.get_images(full=True):
        xref = img_info[0]
        base_image = doc.extract_image(xref)
        width, height = base_image["width"], base_image["height"]

        # Filter out small/decorative images
        if width < MIN_WIDTH or height < MIN_HEIGHT:
            continue
        if width * height < MIN_AREA:
            continue
        # Skip very wide but short images (likely horizontal rules)
        if width > 5 * height:
            continue

        images.append({
            "xref": xref,
            "page": page_num + 1,
            "width": width,
            "height": height,
            "format": base_image["ext"],
            "data": base_image["image"],
        })
```

### Pattern 4: Resumable Backfill Processing
**What:** Track processing state for interruptible backfill
**When to use:** During the full 711-meeting backfill

```python
# Use a local JSON file to track progress (not DB -- faster, no network)
PROGRESS_FILE = "backfill_progress.json"

def load_progress():
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE) as f:
            return json.load(f)
    return {"processed": [], "errors": [], "started_at": None}

def save_progress(state):
    with open(PROGRESS_FILE, "w") as f:
        json.dump(state, f, indent=2)
```

### Anti-Patterns to Avoid
- **Sending entire 700-page PDFs to Gemini in one request:** Gemini supports up to 1000 pages, but very large PDFs may hit the 50MB inline limit. Use the File API for PDFs over 50MB, or chunk at natural TOC boundaries.
- **Running Docling on the entire agenda package:** Process page ranges individually (per document boundary) for better section granularity and error isolation.
- **Re-implementing heading detection in Docling output:** Docling already provides `##` heading hierarchy in markdown; parse the markdown rather than re-analyzing fonts.
- **Storing raw image bytes in Supabase:** Images go to R2; store only metadata (URL, dimensions, page) in the database.

## Discretion Recommendations

### PDF Splitting Strategy for Gemini (Claude's Discretion)
**Recommendation: Simple page-count chunking with agenda overlap**

Most View Royal agenda PDFs are under 300 pages (well within Gemini's 1000-page/50MB limit). For the rare 700+ page outliers:
1. Check PDF file size and page count
2. If under 50MB and under 1000 pages: send as single inline request
3. If 50MB-100MB: upload via File API, send as single request
4. If over 1000 pages: split into ~500-page chunks with 2-page overlap, send the first few agenda/TOC pages with every chunk (so Gemini can always link to agenda items)

**Rationale:** The proven test scripts show Gemini handles 30 pages at ~$0.001 cost. At 258 tokens/page, a 700-page PDF uses ~180K tokens -- well within the 1M token context window. The main constraint is the 50MB file size limit, not token count.

### Agenda Item Linking Approach (Claude's Discretion)
**Recommendation: Gemini-native linking in boundary detection**

Gemini should identify the agenda_item mapping directly in its boundary detection response (e.g., `"agenda_item": "5a"` for a delegation, `"agenda_item": "6.1a"` for a staff report). This replaces the current 4-strategy heuristic approach (`link_sections_to_agenda_items` in `document_chunker.py`).

For the rare cases where Gemini cannot determine the agenda item (e.g., appendices with no clear parent), fall back to the existing fuzzy matching strategies.

**Rationale:** Gemini sees the entire PDF including the TOC/agenda pages with page references like "Pg.52 - 64". It can directly map "Staff Report: Development Variance Permit 2025-04" at pages 52-64 to agenda item "6.1a". This is far more reliable than post-hoc title matching.

### Database Schema Design (Claude's Discretion)
**Recommendation: Add an `extracted_documents` table between `documents` and `document_sections`**

```sql
CREATE TABLE extracted_documents (
    id bigint generated by default as identity primary key,
    document_id bigint REFERENCES documents(id) ON DELETE CASCADE NOT NULL,
    agenda_item_id bigint REFERENCES agenda_items(id) ON DELETE SET NULL,
    title text NOT NULL,
    document_type text NOT NULL DEFAULT 'other',  -- agenda/minutes/staff_report/delegation/...
    page_start integer,
    page_end integer,
    summary text,
    key_facts jsonb,
    municipality_id bigint REFERENCES municipalities(id) DEFAULT 1,
    created_at timestamptz DEFAULT timezone('utc'::text, now()) NOT NULL
);
```

Then modify `document_sections` to reference `extracted_documents` instead of (or in addition to) `documents`:
- Add `extracted_document_id bigint REFERENCES extracted_documents(id) ON DELETE CASCADE` to `document_sections`
- Keep the existing `document_id` FK for backward compatibility

**Rationale:** The natural hierarchy is: `documents` (source PDF files) -> `extracted_documents` (individual documents within a PDF) -> `document_sections` (sections within a document). This matches the real-world structure and enables querying like "show me all staff reports" or "find delegations about topic X". The Gemini output naturally produces this intermediate level.

### PyMuPDF Font-Analysis Fallback (Claude's Discretion)
**Recommendation: Keep as fallback for non-agenda PDFs and scanned PDFs**

The existing `document_chunker.py` should be preserved but demoted to a fallback:
1. **Primary path (new):** Gemini boundary detection + Docling extraction (for agenda PDFs)
2. **Fallback path (existing):** PyMuPDF font-analysis chunking (for non-agenda individual PDFs, or when Gemini/Docling fail)
3. **OCR fallback (existing):** marker-pdf for scanned PDFs where both primary and fallback fail

**Rationale:** Some municipalities may have individual attachment PDFs (not bundled in the agenda package). These small single-document PDFs don't need Gemini boundary detection -- the existing chunker works fine for them.

### Table Storage Approach (Claude's Discretion)
**Recommendation: Inline markdown tables in section_text**

Docling already renders tables as markdown tables in its output. Store them inline within `section_text`. Do NOT create a separate tables table.

**Rationale:** Tables in context are more useful for RAG/search than isolated tables. The embedding model can capture table content when it's inline. Creating a separate structured table store adds complexity without clear benefit at this stage.

### Individual Attachments Beyond Main Agenda PDF (Claude's Discretion)
**Recommendation: Process only the main agenda package PDF, not individual attachments**

The archive structure has one main agenda PDF per meeting (the bundled package). Some meetings may have addendum PDFs or supplementary PDFs. For this phase:
1. Process the main agenda PDF with the full Gemini+Docling pipeline
2. Process addendum/supplementary PDFs with the existing PyMuPDF chunker (fallback path)
3. Do NOT attempt to split or re-combine individual attachment files

**Rationale:** The main agenda package PDF is the primary data source (722 of them across 711 meetings). Individual attachments are relatively rare and the existing chunker handles them adequately.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| PDF heading detection | Font-size threshold heuristics | Docling's ML-based layout analysis | Docling handles inconsistent fonts, column layouts, and complex headers that PyMuPDF heuristics miss |
| Document boundary detection | Page-number pattern matching or TOC parsing | Gemini 2.0 Flash multimodal analysis | Gemini understands document structure semantically, handles missing/inconsistent TOC entries |
| Table extraction | Regex-based table parsing | Docling's TableFormer model | TableFormer handles complex table layouts, merged cells, spanning headers |
| Image classification (meaningful vs decorative) | ML-based image classifier | Dimension-based filtering + Gemini metadata | Simple dimension thresholds (100x100px minimum, skip very wide/short) catch most decorative images; Gemini can provide type classification in its boundary output |
| Rate limiting for Gemini API | Custom retry/backoff logic | Simple `time.sleep()` between requests | At 711 meetings, even 1 request/second completes in 12 minutes; no need for complex rate limiting |

**Key insight:** The combination of Gemini (semantic understanding) + Docling (structural extraction) + PyMuPDF (image extraction) covers all the cases that the current PyMuPDF-only approach struggles with. Each tool does what it's best at.

## Common Pitfalls

### Pitfall 1: Gemini Output Token Limit
**What goes wrong:** Gemini 2.0 Flash has an 8192 output token limit. For a 700-page PDF, the boundary detection JSON response could be very long.
**Why it happens:** Many documents (50+) in a large agenda package, each needing title, summary, key_facts.
**How to avoid:** Keep the Gemini prompt focused on metadata only (not content extraction). Request compact JSON. For very large PDFs, chunk and merge results.
**Warning signs:** `finish_reason` is `MAX_TOKENS` instead of `STOP` in the response.

### Pitfall 2: Docling Page Range Off-By-One
**What goes wrong:** Docling `page_range` uses 1-indexed inclusive ranges, but there's a known issue where the final page may not be processed correctly (GitHub issue #1469).
**Why it happens:** Bug in Docling's page range handling.
**How to avoid:** Add 1 to page_end when passing to Docling, or verify output includes expected final page content. Test with boundary cases.
**Warning signs:** Last page of a document section is missing content.

### Pitfall 3: Gemini 2.0 Flash Retirement
**What goes wrong:** Gemini 2.0 Flash is being retired on March 31, 2026.
**Why it happens:** Google's model lifecycle policy.
**How to avoid:** Design the Gemini prompt and response parsing to be model-agnostic. Use a config variable for the model name. The same prompt should work with Gemini 2.5 Flash or Gemini 3 Flash.
**Warning signs:** Date approaching March 2026; test with newer models before retirement.

### Pitfall 4: Large PDF File Size Exceeds Inline Limit
**What goes wrong:** PDFs over 50MB can't be sent inline to Gemini. The API returns an error.
**Why it happens:** Agenda packages with many high-resolution images or scanned pages.
**How to avoid:** Check file size before sending. Use `client.files.upload()` for PDFs over ~40MB (with safety margin). Files uploaded via File API are available for 48 hours.
**Warning signs:** HTTP 400 errors mentioning payload size.

### Pitfall 5: Docling Model Loading Time
**What goes wrong:** First Docling conversion takes 10-30 seconds for model loading (TableFormer, layout analysis).
**Why it happens:** Docling loads ML models on first use.
**How to avoid:** Initialize `DocumentConverter` once and reuse across all documents. Call `converter.initialize_pipeline(InputFormat.PDF)` at startup. Set `OMP_NUM_THREADS=4` for consistent CPU usage.
**Warning signs:** First document takes 30+ seconds, subsequent ones take 1-5 seconds.

### Pitfall 6: R2 Image Upload Without Wrangler Binding
**What goes wrong:** The pipeline runs locally but needs to upload images to Cloudflare R2.
**Why it happens:** R2 is typically accessed via Worker bindings, but the pipeline is a Python script.
**How to avoid:** Use the R2 S3-compatible API with `boto3` or the Cloudflare API directly. Create an API token with R2 write access.
**Warning signs:** No R2 binding in wrangler.toml; no S3 endpoint configured.

### Pitfall 7: Existing document_sections Data Must Be Fully Replaced
**What goes wrong:** Partial replacement leaves orphaned or duplicate sections.
**Why it happens:** The "delete and replace" strategy requires deleting ALL existing sections before inserting new ones.
**How to avoid:** Delete all `document_sections` rows at the start of backfill (single SQL `DELETE FROM document_sections`). Then process all meetings. Run embedding generation as a final pass.
**Warning signs:** Section counts don't match expected output; duplicate section_titles in same document.

## Code Examples

### Gemini Boundary Detection Prompt (Proven Working)
```python
# Source: /tmp/gemini_test.py, adapted for metadata-only extraction
BOUNDARY_PROMPT = """You are analyzing a municipal council meeting agenda package PDF.
This PDF contains a table of contents/agenda listing at the start, then the actual
documents referenced by the agenda (minutes, staff reports, delegation materials, etc.).

Your task: Extract the document structure as a JSON array. For each distinct
document/section in this PDF, provide:

{
  "title": "Document heading or title",
  "page_start": 1,
  "page_end": 5,
  "type": "staff_report",
  "agenda_item": "6.1a",
  "summary": "1-2 sentence summary",
  "key_facts": ["Important data points"],
  "images": [
    {
      "page": 3,
      "description": "Site plan showing proposed development at 1701 Island Highway",
      "type": "diagram"
    }
  ]
}

Guidelines:
- type: one of agenda, minutes, staff_report, delegation, correspondence, appendix, table, other
- agenda_item: the agenda item number this document relates to (from the TOC), or null
- Be precise about page boundaries
- Each distinct document should be its own entry
- Don't split sub-sections within a single document
- key_facts: capture dollar amounts, dates, addresses, decisions
- images: describe meaningful charts, maps, diagrams, renderings (skip logos/signatures)

Return ONLY a JSON array, no other text."""
```

### Docling Converter Initialization (Proven Working)
```python
# Source: /tmp/test_docling.py
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
import os

os.environ.setdefault("OMP_NUM_THREADS", "4")

def create_converter() -> DocumentConverter:
    """Create and initialize Docling converter (reuse across all documents)."""
    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = False
    pipeline_options.generate_page_images = False
    pipeline_options.generate_picture_images = False  # We use PyMuPDF

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
        }
    )
    converter.initialize_pipeline(InputFormat.PDF)
    return converter
```

### Existing Section Insertion Pattern (From ingester.py)
```python
# Source: apps/pipeline/pipeline/ingestion/ingester.py, _ingest_document_sections()
rows = []
for section in sections:
    rows.append({
        "document_id": doc_id,
        "agenda_item_id": section.get("agenda_item_id"),
        "section_title": section.get("section_title"),
        "section_text": section["section_text"],
        "section_order": section["section_order"],
        "page_start": section.get("page_start"),
        "page_end": section.get("page_end"),
        "token_count": section.get("token_count"),
        "municipality_id": self.municipality_id,
    })

# Insert in batches of 50 to avoid payload size limits
batch_size = 50
for i in range(0, len(rows), batch_size):
    batch = rows[i : i + batch_size]
    self.supabase.table("document_sections").insert(batch).execute()
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| PyMuPDF font-size heuristics | Docling ML-based layout analysis | 2025 (Docling v2+) | Better heading detection, table extraction, handles complex layouts |
| Manual agenda item linking (4 strategies) | Gemini multimodal document understanding | 2025 (Gemini 2.0 Flash) | Direct semantic linking from TOC references; no heuristic fallback needed |
| No image extraction | PyMuPDF + R2 storage | New in this phase | First-time image extraction and storage |
| Single-document processing | Agenda package boundary detection | New in this phase | Correct document-level granularity (staff reports, delegations, etc.) |

**Deprecated/outdated:**
- Gemini 2.0 Flash: Retiring March 31, 2026. Design for easy model swap to 2.5 Flash or 3 Flash.
- Docling page_range bug (issue #1469): May be fixed in future versions. Add workaround.

## Existing Codebase Analysis

### Current Database State
- **711 meetings** with `has_agenda = true` (municipality_id = 1)
- **7 documents** ingested (only 2 meetings have documents so far -- very early)
- **293 document_sections** across those 7 documents
- `document_sections` schema: id, document_id, agenda_item_id, section_title, section_text, section_order, page_start, page_end, token_count, embedding, text_search, municipality_id, created_at
- `documents` schema: id, meeting_id, title, category, source_url, file_path, file_hash, full_text, page_count, embedding, municipality_id, created_at, updated_at
- Match function `match_document_sections()` exists for vector search

### Files To Modify
1. **`apps/pipeline/pipeline/ingestion/ingester.py`** - `_ingest_document_sections()` method (lines 769-855): Replace call to `chunk_document` with new Gemini+Docling extractor
2. **`apps/pipeline/pipeline/ingestion/document_chunker.py`** - Keep as fallback module but refactor `link_sections_to_agenda_items` to accept Gemini-provided agenda_item mappings
3. **`apps/pipeline/pipeline/orchestrator.py`** - `backfill_document_sections()` method (lines 435-558): Update to use new pipeline, add resumable tracking
4. **`apps/pipeline/pipeline/ingestion/embed.py`** - No changes needed; already handles `document_sections` table

### Files To Create
1. `apps/pipeline/pipeline/ingestion/document_extractor.py` - Orchestrator for the three-tool pipeline
2. `apps/pipeline/pipeline/ingestion/gemini_boundary.py` - Gemini boundary detection
3. `apps/pipeline/pipeline/ingestion/docling_extractor.py` - Docling content extraction
4. `apps/pipeline/pipeline/ingestion/image_extractor.py` - PyMuPDF image extraction + R2 upload

### Web App Impact
- `apps/web/app/routes/document-viewer.tsx` - Reads `document_sections` (no changes needed)
- `apps/web/app/services/meetings.ts` - Queries `document_sections` by document_id (no changes needed)
- Both web queries select: `id, document_id, agenda_item_id, section_title, section_text, section_order, page_start, page_end, token_count` -- all existing columns, no breaking changes

### Gemini API Details (Verified)
- **Model:** `gemini-2.0-flash` (retiring March 31, 2026)
- **Context window:** 1,048,576 tokens input
- **Output limit:** 8,192 tokens
- **PDF support:** Up to 50MB inline, up to 1000 pages
- **File API:** For PDFs over 50MB; files stored 48 hours
- **Token cost per page:** ~258 tokens (image modality)
- **Pricing:** $0.10/1M input tokens, $0.40/1M output tokens
- **Estimated backfill cost:** ~65K pages * 258 tokens/page = ~16.8M input tokens = ~$1.68 input + output overhead = ~$6-10 total
- **Rate limits (Tier 1):** 10M tokens/min batch enqueued

### Docling API Details (Verified)
- **Version:** 2.73.1 (current, Feb 2026)
- **Page range:** `converter.convert(source=path, page_range=(start, end))` -- 1-indexed inclusive
- **Max pages:** `max_num_pages` parameter available
- **Export:** `result.document.export_to_markdown()` for full markdown
- **Item iteration:** `result.document.iterate_items()` yields (item, level) tuples
- **Table modes:** `TableFormerMode.FAST` or `TableFormerMode.ACCURATE` (default)
- **Performance:** 30 pages in 20.9s (from test script); ~0.7s/page
- **Model loading:** First conversion takes 10-30s; subsequent ~0.7s/page
- **Known issue:** page_range may not process final page correctly (GitHub #1469)

### R2 Setup Required
- No R2 binding currently in `wrangler.toml`
- Need to create R2 bucket for document images
- Pipeline accesses R2 via S3-compatible API (not Worker binding)
- Add `boto3` dependency for S3-compatible R2 uploads from Python pipeline

## Open Questions

1. **R2 bucket naming and access pattern**
   - What we know: Images go to R2 for edge serving; pipeline uploads via S3 API
   - What's unclear: Bucket name, folder structure, public access URL pattern
   - Recommendation: Create `viewroyal-document-images` bucket; structure as `{meeting_id}/{extracted_doc_id}/{image_hash}.{ext}`; configure public access via custom domain or R2 public URL

2. **Gemini model migration timeline**
   - What we know: Gemini 2.0 Flash retires March 31, 2026 (6 weeks away)
   - What's unclear: Whether the backfill will be complete before retirement
   - Recommendation: Use `gemini-2.0-flash` for immediate backfill (fastest to ship); add model name as a config variable for easy swap to `gemini-2.5-flash` or `gemini-3-flash`

3. **Handling PDFs with no text (scanned-only)**
   - What we know: Some older PDFs are scanned images with no extractable text
   - What's unclear: How many of the 711 agenda PDFs are scanned
   - Recommendation: Check if Docling extraction yields empty output; fall back to `pipeline_options.do_ocr = True` or existing marker-pdf OCR path

## Sources

### Primary (HIGH confidence)
- Existing codebase: `apps/pipeline/pipeline/ingestion/document_chunker.py` - Current pipeline implementation
- Existing codebase: `apps/pipeline/pipeline/ingestion/ingester.py` - Integration point
- Test scripts: `/tmp/gemini_test.py`, `/tmp/gemini_test_structured.py`, `/tmp/test_docling.py` - Proven working
- Test output: `/tmp/gemini_structured_output.json` - Gemini output format verified
- Test output: `/tmp/docling_output.md` - Docling output format verified
- Database: Supabase `document_sections` schema - 293 existing rows, schema confirmed
- [Gemini Document Processing](https://ai.google.dev/gemini-api/docs/document-processing) - PDF limits: 50MB/1000 pages, 258 tokens/page
- [Docling GitHub](https://github.com/docling-project/docling) - v2.73.1, page_range parameter
- [Docling DocumentConverter API](https://docling-project.github.io/docling/reference/document_converter/) - convert() parameters

### Secondary (MEDIUM confidence)
- [Gemini Rate Limits](https://ai.google.dev/gemini-api/docs/rate-limits) - Tier-dependent, batch API available at 50% cost
- [Gemini Batch API](https://ai.google.dev/gemini-api/docs/batch-api) - 50% cost reduction, 24h turnaround
- [Docling Advanced Options](https://docling-project.github.io/docling/usage/advanced_options/) - Pipeline options, OMP_NUM_THREADS
- [Docling page_range issue #1469](https://github.com/docling-project/docling/issues/1469) - Known final-page bug
- [PyMuPDF Image Recipes](https://pymupdf.readthedocs.io/en/latest/recipes-images.html) - extract_image, get_images API
- [Gemini 2.0 Flash retirement](https://ai.google.dev/gemini-api/docs/models) - March 31, 2026

### Tertiary (LOW confidence)
- R2 S3-compatible API access pattern from Python - Needs validation with actual credentials

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - All libraries already in pyproject.toml with proven test scripts
- Architecture: HIGH - Clear separation of concerns; proven API patterns; minimal schema changes
- Pitfalls: HIGH - Well-documented API limits; known issues catalogued; retirement timeline clear
- R2 integration: MEDIUM - Pattern is standard (S3-compatible) but not yet validated in this codebase

**Research date:** 2026-02-17
**Valid until:** 2026-03-15 (Gemini 2.0 Flash retirement on 2026-03-31 is a hard deadline)
