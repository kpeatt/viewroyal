---
phase: 10-add-better-test-suite
plan: 05
type: execute
wave: 3
depends_on: ["10-02", "10-03", "10-04"]
files_modified:
  - scripts/pre-deploy.sh
  - scripts/test-all.sh
  - apps/pipeline/tests/integration/test_ingest_meeting.py
  - apps/web/package.json
autonomous: true
requirements: []

must_haves:
  truths:
    - "Running `./scripts/pre-deploy.sh` executes both pipeline and web tests, failing on any test failure"
    - "Running `./scripts/test-all.sh` runs both test suites with optional arguments"
    - "Integration test exercises MeetingIngester.process_meeting() end-to-end with all external services mocked"
    - "The `pnpm deploy` script in apps/web is gated by pre-deploy test pass"
    - "Pipeline coverage report shows 80%+ on tested modules (ingestion, core)"
  artifacts:
    - path: "scripts/pre-deploy.sh"
      provides: "Pre-deploy test gate script that runs both pipeline and web tests"
      min_lines: 20
    - path: "scripts/test-all.sh"
      provides: "Convenience script to run all tests with pass-through args"
      min_lines: 15
    - path: "apps/pipeline/tests/integration/test_ingest_meeting.py"
      provides: "Phase-level integration test for meeting ingestion"
      min_lines: 60
  key_links:
    - from: "scripts/pre-deploy.sh"
      to: "apps/pipeline/pytest.ini"
      via: "Runs uv run pytest with pipeline config"
      pattern: "uv run pytest"
    - from: "scripts/pre-deploy.sh"
      to: "apps/web/package.json"
      via: "Runs pnpm test run"
      pattern: "pnpm test run"
    - from: "apps/pipeline/tests/integration/test_ingest_meeting.py"
      to: "apps/pipeline/pipeline/ingestion/ingester.py"
      via: "Exercises process_meeting with mocked externals"
      pattern: "process_meeting"
---

<objective>
Create the integration test for meeting ingestion, pre-deploy hook scripts, and verify overall coverage meets the 80%+ target on tested modules. Wire up deploy gating.

Purpose: Complete the test suite with integration testing and workflow hooks that prevent broken deploys.
Output: Integration test, pre-deploy and test-all scripts, deploy gating in package.json.
</objective>

<execution_context>
@/Users/kyle/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kyle/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-add-better-test-suite/10-RESEARCH.md
@.planning/phases/10-add-better-test-suite/10-01-SUMMARY.md
@.planning/phases/10-add-better-test-suite/10-03-SUMMARY.md
@.planning/phases/10-add-better-test-suite/10-04-SUMMARY.md

@apps/pipeline/pipeline/ingestion/ingester.py
@apps/pipeline/tests/conftest.py
@apps/web/package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integration test for meeting ingestion</name>
  <files>
    apps/pipeline/tests/integration/test_ingest_meeting.py
  </files>
  <action>
    1. **Read `pipeline/ingestion/ingester.py`** fully to understand `process_meeting()`:
       - What external services does it call? (Supabase reads/writes, Gemini refinement, file I/O, VimeoClient)
       - What is the method signature and expected flow?
       - What does a successful run produce? (Supabase inserts for meeting, agenda_items, motions, votes, transcript_segments, etc.)

    2. **Create `tests/integration/__init__.py`**.

    3. **Create `tests/integration/test_ingest_meeting.py`:**
       This is the ONE integration test that exercises `process_meeting()` with all externals mocked:

       ```python
       # Conceptual structure:
       class TestIngestMeeting:
           @pytest.fixture
           def ingester(self, mock_supabase):
               """Create MeetingIngester with mocked Supabase."""
               # Patch create_client to return our mock
               with patch("pipeline.ingestion.ingester.create_client", return_value=mock_supabase):
                   ing = MeetingIngester("http://test.url", "test-key", gemini_key="test")
                   # Also mock the gemini_client
                   ing.gemini_client = MagicMock()
                   return ing

           def test_process_meeting_happy_path(self, ingester, tmp_archive_dir, meeting_693_data):
               """Full meeting ingestion with pre-loaded fixture data."""
               # Set up: create fixture files in tmp_archive_dir
               # - agenda.md, minutes.md (from fixture data or minimal text)
               # - refinement.json (from fixture data)
               # Mock Supabase reads to return fixture data
               # Mock Gemini to return the fixture refinement
               # Call process_meeting()
               # Assert: Supabase .upsert() or .insert() called for meetings, agenda_items, etc.
               pass

           def test_process_meeting_no_documents(self, ingester, tmp_archive_dir):
               """Meeting with no PDF documents should still process."""
               pass

           def test_process_meeting_gemini_failure(self, ingester, tmp_archive_dir):
               """When Gemini fails, should fall back to agenda-only processing."""
               pass

           def test_process_meeting_missing_transcript(self, ingester, tmp_archive_dir):
               """Meeting without transcript data should process agenda/minutes only."""
               pass
       ```

       - Use `tmp_archive_dir` fixture from conftest.py for file I/O
       - Use `meeting_693_data` fixture for realistic data
       - Mock all Supabase table operations to track what was inserted
       - Mock Gemini's `generate_content` to return fixture refinement data
       - Mock VimeoClient if video URL extraction is part of process_meeting
       - Assert the RIGHT Supabase operations happened (meeting upserted, items inserted, etc.)
       - Test error paths: Gemini failure, missing transcript, no documents

    4. Create `tests/integration/` directory with `__init__.py`.
  </action>
  <verify>
    `cd apps/pipeline && uv run pytest tests/integration/ -v`
    All integration tests pass. At least 4 tests.
    No external service calls (verified by: tests pass without network/API keys).
  </verify>
  <done>Integration test exercises full meeting ingestion with mocked externals, covering happy path and error scenarios.</done>
</task>

<task type="auto">
  <name>Task 2: Pre-deploy hooks, test-all script, deploy gating, and coverage verification</name>
  <files>
    scripts/pre-deploy.sh
    scripts/test-all.sh
    apps/web/package.json
  </files>
  <action>
    1. **Create `scripts/pre-deploy.sh`:**
       ```bash
       #!/bin/bash
       set -e

       REPO_DIR="$(cd "$(dirname "$0")/.." && pwd)"
       PIPELINE_DIR="$REPO_DIR/apps/pipeline"
       WEB_DIR="$REPO_DIR/apps/web"

       echo "================================"
       echo "  Pre-deploy Test Gate"
       echo "================================"
       echo ""

       echo "[1/2] Pipeline tests..."
       cd "$PIPELINE_DIR"
       uv run pytest --tb=short -q --ignore=tests/core/test_marker_ocr.py
       echo ""

       echo "[2/2] Web server tests..."
       cd "$WEB_DIR"
       pnpm test run 2>&1
       echo ""

       echo "================================"
       echo "  All tests passed. Safe to deploy."
       echo "================================"
       ```
       Make executable: `chmod +x scripts/pre-deploy.sh`

    2. **Create `scripts/test-all.sh`:**
       ```bash
       #!/bin/bash
       set -e

       REPO_DIR="$(cd "$(dirname "$0")/.." && pwd)"
       PIPELINE_DIR="$REPO_DIR/apps/pipeline"
       WEB_DIR="$REPO_DIR/apps/web"

       echo "=== Pipeline Tests ==="
       cd "$PIPELINE_DIR"
       uv run pytest "$@"
       echo ""

       echo "=== Web Tests ==="
       cd "$WEB_DIR"
       pnpm test run
       echo ""

       echo "All tests passed."
       ```
       Make executable: `chmod +x scripts/test-all.sh`

    3. **Update `apps/web/package.json`** to gate deploy on tests:
       - Find the existing `deploy` script (should be something like `"deploy": "pnpm build && wrangler deploy"`)
       - Change to: `"predeploy": "vitest run"` -- npm/pnpm runs `pre<script>` automatically before `<script>`
       - OR change deploy to: `"deploy": "vitest run && pnpm build && wrangler deploy"`
       - Whichever pattern the existing package.json uses, add the test gate

    4. **Run full coverage report and verify:**
       ```bash
       cd apps/pipeline && uv run pytest --cov=pipeline --cov-report=term-missing --ignore=tests/core/test_marker_ocr.py
       ```
       Check the output -- the ingestion, core, profiling, scrapers, and video modules should show substantial coverage. Note the total coverage percentage.

       ```bash
       cd apps/web && pnpm test:coverage
       ```
       Check web app coverage on intent.ts, supabase.server.ts, meetings.ts.

    5. Print a summary of:
       - Total pipeline tests count
       - Total web app tests count
       - Pipeline coverage percentage on key modules
       - Web coverage percentage on covered files
  </action>
  <verify>
    `./scripts/pre-deploy.sh` -- runs both test suites and reports success.
    `./scripts/test-all.sh` -- runs all tests with arguments passed through.
    `cd apps/pipeline && uv run pytest --cov=pipeline --cov-report=term-missing` -- coverage report visible.
    `cd apps/web && pnpm test run` -- web tests pass.
    Pipeline coverage on ingestion/ and core/ modules is 60%+ (80% target is aspirational for the full pipeline).
  </verify>
  <done>Pre-deploy hooks created and working. Deploy gated on test pass. Coverage reports generated for both pipeline and web app.</done>
</task>

</tasks>

<verification>
1. `./scripts/pre-deploy.sh` -- exits 0 when all tests pass
2. `./scripts/test-all.sh` -- runs full suite with optional args
3. `cd apps/pipeline && uv run pytest -v` -- all tests pass, coverage visible
4. `cd apps/web && pnpm test run` -- all web tests pass
5. Integration test exercises process_meeting() without hitting real services
6. Coverage report shows meaningful coverage on ingestion, core, and profiling modules
</verification>

<success_criteria>
- Pre-deploy script runs both test suites and blocks deploy on failure
- Test-all convenience script works with argument passthrough
- Deploy script in package.json gated on test pass
- Integration test covers happy path + 3 error scenarios for meeting ingestion
- Coverage reports generated and showing results for both pipeline and web app
- Total test count: 60+ tests across pipeline and web app
</success_criteria>

<output>
After completion, create `.planning/phases/10-add-better-test-suite/10-05-SUMMARY.md`
</output>
