# Architecture Research: v1.1 Deep Intelligence

**Research date:** 2026-02-16
**Scope:** How document sectioning, hybrid RAG, conversation memory, and council profiling integrate with existing architecture

---

## 1. Current Architecture Summary

```
Python ETL Pipeline (apps/pipeline/)
  Phase 1: Scrape (CivicWeb PDFs)
  Phase 2: Download Audio (Vimeo)
  Phase 3: Diarize (MLX, local)
  Phase 4: AI Refine + Ingest → Supabase
  Phase 5: Embed (fastembed) → pgvector

Supabase PostgreSQL
  Tables: meetings, agenda_items, motions, transcript_segments,
          documents, key_statements, people, votes, matters, ...
  Extensions: pgvector (halfvec 384), tsvector FTS
  Edge Functions: send-alerts (email notifications)

Cloudflare Workers (apps/web/)
  React Router 7 SSR
  Services: meetings.ts, matters.ts, site.ts, rag.server.ts, ...
  RAG: query embedding (OpenAI) → vector search (Supabase RPC) → Gemini synthesis
```

---

## 2. Integration Points for New Features

### 2a. Document Sectioning — Pipeline Modification

**Where it fits**: Between existing Phase 4 (Ingest) and Phase 5 (Embed).

**Current flow** (ingester.py `_ingest_documents()`):
```
PDF files in Agenda/ and Minutes/ subfolders
  → PyMuPDF extract full text
  → Classify by filename (Agenda, Addendum, Supplementary, etc.)
  → Upsert to `documents` table (full_text, file_hash, category)
  → Single embedding per document in Phase 5
```

**New flow**:
```
PDF files in Agenda/ and Minutes/ subfolders
  → PyMuPDF extract full text (existing)
  → Upsert to `documents` table (existing)
  → NEW: Parse document structure (headings, sections, page breaks)
  → NEW: Chunk into sections (heading-based or fixed-size fallback)
  → NEW: Upsert sections to `document_sections` table
  → Phase 5: Embed document sections (in addition to existing key_statements)
```

**Modified files**:
- `apps/pipeline/pipeline/ingestion/ingester.py` — Add `_ingest_document_sections()` method after `_ingest_documents()`
- `apps/pipeline/pipeline/embeddings/embed.py` — Add document_sections embedding alongside key_statements
- New: chunking utility in `apps/pipeline/pipeline/ingestion/document_chunker.py`

**Schema addition**:
```sql
CREATE TABLE document_sections (
    id bigint generated by default as identity primary key,
    document_id bigint references documents(id) on delete cascade not null,
    section_title text,
    section_text text not null,
    section_order integer not null,
    page_start integer,
    page_end integer,
    token_count integer,
    embedding halfvec(384),
    search_text tsvector generated always as (to_tsvector('english', section_text)) stored,
    municipality_id bigint references municipalities(id) default 1,
    created_at timestamptz default now() not null
);

CREATE INDEX idx_document_sections_embedding ON document_sections
    USING ivfflat (embedding halfvec_cosine_ops) WITH (lists = 100);
CREATE INDEX idx_document_sections_search ON document_sections USING gin(search_text);
CREATE INDEX idx_document_sections_document ON document_sections(document_id);
```

### 2b. Hybrid RAG Search — RPC + Service Layer

**Where it fits**: `apps/web/app/services/rag.server.ts` — the existing RAG pipeline.

**Current RAG flow**:
```
User question → OpenAI embedding (384d) → match_key_statements RPC → top results
                                        → (match_transcript_segments — broken)
  → Concatenate context
  → Gemini system prompt + context + question
  → Streaming answer with citations
```

**New RAG flow**:
```
User question → OpenAI embedding (384d)
              → hybrid_search RPC:
                  1. Vector search: key_statements, document_sections, motions (by embedding)
                  2. FTS search: transcript_segments, document_sections, motions (by tsvector)
                  3. Reciprocal Rank Fusion to merge results
                  4. Return top-N with source type + metadata
              → Concatenate context (with source attribution)
              → Gemini system prompt + conversation history + context + question
              → Streaming answer with citations
```

**Modified files**:
- `apps/web/app/services/rag.server.ts` — Replace individual RPC calls with `hybrid_search`, add conversation context
- New RPC: `hybrid_search(query_embedding halfvec, query_text text, match_count int, municipality_id bigint)`
- `apps/web/app/services/vectorSearch.ts` — Update to use hybrid search

### 2c. Conversation Memory — Client + Server

**Where it fits**: New table + modifications to Ask page and RAG service.

**Architecture**:
```
Browser (Ask page)
  → sessionStorage: conversationId (UUID, created on page load)
  → On question submit: POST with conversationId + question

Server (api.ask.tsx action)
  → Load recent turns from rag_conversations where session_id = conversationId
  → Pass to rag.server.ts as conversation context
  → After answer: save Q&A turn to rag_conversations

rag.server.ts
  → Build Gemini prompt:
      System: "You are a research agent for {municipality}..."
      Previous turns: [{role: user, content: Q1}, {role: assistant, content: A1}, ...]
      Context: [search results]
      Current question: Q2
  → Stream answer
```

**Modified files**:
- `apps/web/app/routes/api.ask.tsx` — Add conversationId parameter, load/save turns
- `apps/web/app/services/rag.server.ts` — Accept conversation history, include in Gemini prompt
- `apps/web/app/routes/ask.tsx` — Generate sessionId, track conversation state, show conversation thread
- New migration for `rag_conversations` table

**Schema**:
```sql
CREATE TABLE rag_conversations (
    id bigint generated by default as identity primary key,
    session_id uuid not null,
    role text check (role in ('user', 'assistant')) not null,
    content text not null,
    sources jsonb, -- citation metadata for assistant responses
    municipality_id bigint references municipalities(id) default 1,
    created_at timestamptz default now() not null
);

CREATE INDEX idx_rag_conversations_session ON rag_conversations(session_id, created_at);
```

### 2d. Council Member Profiling — Pipeline + Web App

**Where it fits**: New pipeline phase (or post-ingestion job) + enhanced person route.

**Data aggregation** (SQL, no AI needed):
```
Voting patterns:
  votes JOIN motions → per-person vote breakdown (yea/nay/abstain by category)
  votes JOIN votes (self-join) → pairwise alignment between councillors

Activity metrics:
  attendance → attendance rate per person
  transcript_segments → speaking time per person (sum of durations)
  motions → motions proposed/seconded per person
  key_statements → topics spoken about per person

All achievable with Supabase queries from the web app service layer.
```

**AI stance summaries** (Gemini, requires caching):
```
Pipeline or scheduled job:
  For each active councillor:
    For each major topic category:
      Query key_statements + votes for person + topic
      Send to Gemini: "Summarize this councillor's position on {topic}"
      Store result in person_stance_summaries table

Web app reads cached summaries. Regenerate after new meeting ingestion.
```

**Modified files**:
- `apps/web/app/routes/person-profile.tsx` — Enhanced loader with voting data, metrics, stances
- `apps/web/app/services/people.ts` — New functions: `getVotingPatterns()`, `getActivityMetrics()`, `getStanceSummaries()`, `getVotingAlignment()`
- New components: `VotingHistory`, `ActivityMetrics`, `StanceSummary`, `AlignmentMatrix`
- Pipeline: new `stance_generator.py` or addition to existing AI refiner post-processing

**Schema**:
```sql
CREATE TABLE person_stance_summaries (
    id bigint generated by default as identity primary key,
    person_id bigint references people(id) on delete cascade not null,
    topic text not null,
    summary text not null,
    supporting_evidence jsonb, -- {statements: [...], votes: [...]}
    confidence text check (confidence in ('high', 'medium', 'low')),
    generated_at timestamptz default now() not null,
    municipality_id bigint references municipalities(id) default 1,
    unique(person_id, topic)
);
```

---

## 3. Build Order (Dependency-Driven)

```
Phase 7: Document Sectioning (Pipeline + Schema)
  ├── document_sections table + migration
  ├── Chunking logic in pipeline
  ├── Embedding generation for sections
  └── Backfill existing documents

Phase 8: Hybrid RAG + Conversation Memory
  ├── hybrid_search RPC function
  ├── Updated rag.server.ts to use hybrid search
  ├── rag_conversations table
  ├── Conversation UI on Ask page
  └── Depends on: Phase 7 (document sections exist)

Phase 9: Council Member Profiling
  ├── Voting pattern SQL queries + components
  ├── Activity metrics aggregation + components
  ├── person_stance_summaries table + generation
  ├── Enhanced person-profile route
  └── Can start in parallel with Phase 8 (voting/metrics don't need hybrid search)

Phase 10: Polish & Integration
  ├── Stance summary generation pipeline integration
  ├── Cross-feature testing (RAG cites document sections, stances use correct data)
  ├── Performance tuning (index optimization, query caching)
  └── Depends on: Phases 7-9 complete
```

**Why this order**:
1. Document sections are foundational — they feed into both RAG and (indirectly) stance summaries via richer data
2. Hybrid RAG depends on document sections existing + having embeddings
3. Council profiling can start the SQL-only parts (voting, metrics) in parallel, but AI stances benefit from the richer data pipeline
4. Polish phase ensures everything works together

---

## 4. Component Boundaries

### Pipeline Boundary
- **Input**: PDF files in archive directories (unchanged)
- **Output**: `documents` + `document_sections` rows in Supabase (expanded)
- **New responsibility**: Chunking documents, embedding sections
- **Unchanged**: Scraping, audio download, diarization, AI refinement

### Web App RAG Boundary
- **Input**: User question + optional conversationId
- **Output**: Streaming answer with citations
- **New responsibility**: Hybrid search orchestration, conversation context management
- **Unchanged**: Gemini synthesis, streaming response format

### Web App Profile Boundary
- **Input**: Person ID from route params
- **Output**: Enhanced profile page with voting, metrics, stances
- **New responsibility**: Aggregation queries, cached AI summaries
- **Unchanged**: Basic person data loading, existing profile layout

---
*Last updated: 2026-02-16*
