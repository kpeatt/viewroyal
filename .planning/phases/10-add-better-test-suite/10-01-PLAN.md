---
phase: 10-add-better-test-suite
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/pipeline/pyproject.toml
  - apps/pipeline/pytest.ini
  - apps/pipeline/tests/conftest.py
  - apps/pipeline/tests/fixtures/meeting_693/meeting.json
  - apps/pipeline/tests/fixtures/meeting_693/agenda_items.json
  - apps/pipeline/tests/fixtures/meeting_693/transcript_segments.json
  - apps/pipeline/tests/fixtures/meeting_693/refinement.json
  - apps/pipeline/tests/fixtures/gemini_responses/stance_generation.json
  - apps/pipeline/tests/pipeline/test_local_refiner_logic.py
autonomous: true
requirements: []

must_haves:
  truths:
    - "Running `uv run pytest` from apps/pipeline/ uses shared fixtures from conftest.py"
    - "Coverage report is generated showing line-by-line coverage of the pipeline package"
    - "Canonical meeting 693 fixture data is available to all tests via pytest fixtures"
    - "The previously-skipped test_merge_refinements test now passes"
    - "All 15+ existing tests continue to pass after infrastructure changes"
  artifacts:
    - path: "apps/pipeline/tests/conftest.py"
      provides: "Shared fixtures: mock_supabase, mock_gemini, meeting_693_data, tmp_archive_dir"
      min_lines: 60
    - path: "apps/pipeline/tests/fixtures/meeting_693/meeting.json"
      provides: "Canonical meeting record for fixture-based testing"
    - path: "apps/pipeline/pytest.ini"
      provides: "Updated pytest config with coverage, markers, and fixture paths"
  key_links:
    - from: "apps/pipeline/tests/conftest.py"
      to: "apps/pipeline/tests/fixtures/"
      via: "json.load for fixture data"
      pattern: "fixtures.*meeting_693"
---

<objective>
Set up pipeline test infrastructure: install new test dependencies (pytest-cov, responses, syrupy, freezegun), create shared conftest.py with reusable mock fixtures, extract canonical meeting 693 fixture data from the live database, configure pytest-cov for coverage reporting, and fix the skipped test_merge_refinements test.

Purpose: Foundation that all subsequent pipeline test plans depend on -- shared mocks, fixtures, and coverage tooling.
Output: Working conftest.py, fixture data files, updated pytest.ini with coverage, fixed skipped test.
</objective>

<execution_context>
@/Users/kyle/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kyle/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-add-better-test-suite/10-RESEARCH.md

@apps/pipeline/pyproject.toml
@apps/pipeline/pytest.ini
@apps/pipeline/tests/pipeline/test_local_refiner_logic.py
@apps/pipeline/pipeline/ingestion/ai_refiner.py
@apps/pipeline/pipeline/ingestion/ingester.py
@apps/pipeline/pipeline/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install test dependencies and configure pytest-cov</name>
  <files>
    apps/pipeline/pyproject.toml
    apps/pipeline/pytest.ini
  </files>
  <action>
    1. Install new test dependencies:
       ```bash
       cd apps/pipeline && uv add --dev pytest-cov responses syrupy freezegun
       ```

    2. Update `pytest.ini` to enable coverage reporting and test markers:
       ```ini
       [pytest]
       pythonpath = .
       testpaths = tests
       python_files = test_*.py
       addopts = -v --tb=short --cov=pipeline --cov-report=term-missing --cov-report=html:coverage_html
       markers =
           slow: marks tests as slow (deselect with '-m "not slow"')
       ```

    3. Add `coverage_html/` to `.gitignore` in `apps/pipeline/` (create if needed, or add to existing). Also add `__snapshots__/` for syrupy.

    4. Verify `uv run pytest --co` (collect-only) succeeds, showing existing tests still discoverable.
  </action>
  <verify>
    `cd apps/pipeline && uv run pytest --co` lists all existing tests without errors.
    `uv run pytest -x` runs existing tests (should pass, coverage output now visible).
  </verify>
  <done>pytest-cov, responses, syrupy, freezegun installed. Coverage reporting enabled. Existing tests unbroken.</done>
</task>

<task type="auto">
  <name>Task 2: Create shared conftest.py, fixture data, and fix skipped test</name>
  <files>
    apps/pipeline/tests/conftest.py
    apps/pipeline/tests/fixtures/meeting_693/meeting.json
    apps/pipeline/tests/fixtures/meeting_693/agenda_items.json
    apps/pipeline/tests/fixtures/meeting_693/transcript_segments.json
    apps/pipeline/tests/fixtures/meeting_693/refinement.json
    apps/pipeline/tests/fixtures/gemini_responses/stance_generation.json
    apps/pipeline/tests/pipeline/test_local_refiner_logic.py
  </files>
  <action>
    1. **Extract fixture data from Supabase** for meeting 693 (2025-11-18 Regular Council). Use the Supabase MCP `execute_sql` tool to query:
       - `meetings` record for id=693 (save as meeting.json)
       - First 5 `agenda_items` with their motions and votes (save as agenda_items.json -- include nested motions array with votes)
       - First 20 `transcript_segments` (save as transcript_segments.json)
       - A representative stance generation response structure (create a realistic sample in gemini_responses/stance_generation.json)

       Also check if there's a refinement.json on disk at `viewroyal_archive/*/` for meeting 693 (search the archive). If found, copy a sanitized version. If not, create a representative sample matching the MeetingRefinement Pydantic model structure.

    2. **Create `tests/conftest.py`** with shared fixtures:
       ```python
       import json
       import os
       import pytest
       from pathlib import Path
       from unittest.mock import MagicMock

       FIXTURES_DIR = Path(__file__).parent / "fixtures"

       @pytest.fixture
       def fixtures_dir():
           return FIXTURES_DIR

       @pytest.fixture
       def meeting_693_data():
           with open(FIXTURES_DIR / "meeting_693" / "meeting.json") as f:
               return json.load(f)

       @pytest.fixture
       def meeting_693_agenda_items():
           with open(FIXTURES_DIR / "meeting_693" / "agenda_items.json") as f:
               return json.load(f)

       @pytest.fixture
       def meeting_693_transcript():
           with open(FIXTURES_DIR / "meeting_693" / "transcript_segments.json") as f:
               return json.load(f)

       @pytest.fixture
       def mock_supabase():
           """Chainable Supabase client mock."""
           client = MagicMock()
           table_mock = MagicMock()
           # All chainable methods return self for fluent API
           for method in ['select', 'eq', 'neq', 'in_', 'not_', 'lte', 'gte', 'lt', 'gt',
                          'or_', 'single', 'insert', 'upsert', 'update', 'delete',
                          'range', 'order', 'limit', 'is_', 'ilike', 'like']:
               getattr(table_mock, method).return_value = table_mock
           table_mock.execute.return_value = MagicMock(data=[], count=0)
           client.table.return_value = table_mock
           # Also support client.rpc()
           rpc_mock = MagicMock()
           rpc_mock.execute.return_value = MagicMock(data=[])
           client.rpc.return_value = rpc_mock
           return client

       @pytest.fixture
       def mock_gemini():
           """Mock Gemini client with generate_content."""
           client = MagicMock()
           response = MagicMock()
           response.parsed = None  # Set per test
           response.text = ""
           client.models.generate_content.return_value = response
           return client

       @pytest.fixture
       def tmp_archive_dir(tmp_path):
           """Create a temporary archive directory structure for a fake meeting."""
           meeting_dir = tmp_path / "2025-11-18_regular-council"
           meeting_dir.mkdir()
           return meeting_dir
       ```

    3. **Fix `test_merge_refinements`** in `tests/pipeline/test_local_refiner_logic.py`:
       - Read the current `SpeakerAlias` model definition in `ai_refiner.py` (it's a Pydantic model, not a dict)
       - Update the test to use `SpeakerAlias(label="Speaker_01", name="Sid Tobias")` instead of `{"label": "Speaker_01", "name": "Sid Tobias"}`
       - Remove the `@pytest.mark.skip` decorator
       - Import `SpeakerAlias` from `pipeline.ingestion.ai_refiner`
       - Run the test to confirm it passes

    4. Create `tests/fixtures/` directory structure with `__init__.py` files if needed for any test imports, and add empty `tests/__init__.py` if not present.
  </action>
  <verify>
    `cd apps/pipeline && uv run pytest tests/pipeline/test_local_refiner_logic.py -v` -- the previously-skipped test now passes.
    `uv run pytest -v` -- all existing tests pass, coverage report is visible.
    `ls tests/fixtures/meeting_693/` -- fixture files exist.
    `python -c "import json; json.load(open('tests/fixtures/meeting_693/meeting.json'))"` -- fixture data is valid JSON.
  </verify>
  <done>Shared conftest.py with mock_supabase, mock_gemini, meeting_693 fixtures created. Skipped test fixed. All tests pass with coverage report.</done>
</task>

</tasks>

<verification>
1. `cd apps/pipeline && uv run pytest -v` -- all tests pass, coverage report visible
2. `uv run pytest tests/pipeline/test_local_refiner_logic.py::test_merge_refinements -v` -- previously-skipped test passes
3. Coverage HTML report exists at `apps/pipeline/coverage_html/index.html`
4. Fixture data is valid JSON and contains realistic meeting data
</verification>

<success_criteria>
- pytest-cov, responses, syrupy, freezegun installed as dev dependencies
- Coverage reporting enabled (term-missing + HTML)
- Shared conftest.py with reusable Supabase/Gemini mocks and meeting 693 fixtures
- Canonical meeting 693 fixture data extracted and saved
- test_merge_refinements fixed and passing
- All existing tests continue to pass
</success_criteria>

<output>
After completion, create `.planning/phases/10-add-better-test-suite/10-01-SUMMARY.md`
</output>
