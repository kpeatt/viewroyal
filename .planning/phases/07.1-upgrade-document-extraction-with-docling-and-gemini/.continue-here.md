---
phase: 07.1-upgrade-document-extraction-with-docling-and-gemini
task: 2
total_tasks: 2
status: paused
last_updated: 2026-02-17T23:15:00.000Z
---

<current_state>
Phase 07.1 execution: Plans 01 and 02 fully complete. Plan 03 task 1 complete (backfill CLI), task 2 is a human-verify checkpoint — paused by user request.

All code is committed (no uncommitted changes). The inline limit fix (9a849cc7) was the last commit. A 10-meeting background test was running (~60% done) when user decided to pause.

**New requirement from user:** The full 711-meeting backfill should use the **Gemini Batch API** instead of sequential synchronous calls. This is a significant architecture change that needs planning before resuming.
</current_state>

<completed_work>

- Plan 07.1-01 (Wave 1): Schema + Gemini extractor — DONE
  - Created `extracted_documents` and `document_images` tables via Supabase migration
  - Added `extracted_document_id` FK to `document_sections`
  - Created `gemini_extractor.py` (438 lines): two-pass extraction with boundary detection + content extraction
  - Commits: bcd42dc9, 4af66f85, 45f9eb07

- Plan 07.1-02 (Wave 2): Image extractor + pipeline integration — DONE
  - Created `image_extractor.py` (PyMuPDF image extraction with R2 upload, dimension filtering)
  - Created `document_extractor.py` (orchestrator: Gemini boundaries -> content -> images -> DB insertion, with fallback to old chunker)
  - Updated `ingester.py` to call new extractor instead of old chunker
  - Updated `orchestrator.py` backfill to use new pipeline
  - Removed Docling from pyproject.toml, added boto3
  - Commits: 3adf2a78, fd9e419a, 52eef13a

- Plan 07.1-03 Task 1: Backfill CLI — DONE
  - Created `backfill_extracted_documents()` method on Archiver class
  - Added `--extract-documents` CLI flag with `--limit` and `--force` support
  - Resumable progress tracking via local JSON file
  - Commit: b7a56c73

- Debugging/fixes (during checkpoint verification):
  - Fixed adaptive PDF splitting for >50MB files (Gemini PDF limit is 50MB, not 2GB)
  - Added page-range extraction for content extraction of large PDFs
  - Bumped inline limit to 50MB to avoid File API rate limits
  - Commits: 7d5e07c5, a1406b55, 9a849cc7

</completed_work>

<remaining_work>

**Before resuming Plan 03 Task 2 checkpoint:**

1. **Research Gemini Batch API** — user wants the full backfill to use batch processing instead of sequential synchronous Gemini calls. This requires:
   - Understanding the Batch API request format (JSON lines, etc.)
   - How to submit batch jobs and poll for results
   - How to map batch results back to individual documents/sections
   - Whether two-pass extraction (boundaries → content) can be batched efficiently
   - Cost/latency tradeoffs

2. **Plan the batch architecture** — likely a new plan (07.1-04?) or a revision of the backfill approach:
   - Prepare all boundary-detection prompts as a batch → submit → wait → parse results
   - Use boundary results to prepare content-extraction prompts as second batch → submit → wait → parse
   - Handle errors/retries for individual items in the batch
   - Update progress tracking for batch-style processing

3. **Execute and verify** — run the batch backfill on all 711 meetings

**After batch backfill works:**
- Approve the Plan 03 human-verify checkpoint
- Phase verification (automatic)
- Roadmap/STATE.md updates

</remaining_work>

<decisions_made>

- Gemini PDF processing limit is 50MB (not 2GB) — confirmed via Google docs
- Inline data payload limit raised to 100MB (Jan 2026 Google update), so PDFs under 50MB always use inline bytes
- Adaptive PDF splitting: for PDFs >50MB, split into chunks targeting 30% of limit (15MB) per chunk
- Fallback to old PyMuPDF chunker kept for Gemini failures
- R2 image uploads gracefully degrade if credentials missing
- **NEW: Full backfill should use Gemini Batch API** (user decision, not yet implemented)

</decisions_made>

<blockers>
- Need to research and plan Gemini Batch API integration before proceeding with full backfill
</blockers>

<context>
The pipeline code is fully functional for synchronous extraction. Small batches (3-10 meetings) work fine with the current sequential approach. The user's new requirement is specifically about the full 711-meeting backfill being more efficient via the Batch API.

Database state: all test extraction data was cleaned up before this pause. extracted_documents and document_sections tables are empty, progress file deleted. Fresh start for next session.

Key files:
- apps/pipeline/pipeline/ingestion/gemini_extractor.py — core Gemini extraction logic
- apps/pipeline/pipeline/ingestion/document_extractor.py — orchestrator
- apps/pipeline/pipeline/orchestrator.py — backfill_extracted_documents method
- apps/pipeline/main.py — CLI entry point

The 10-meeting background test that was running at pause time should be considered abandoned — its results (if any) are ephemeral and the DB was cleaned up.
</context>

<next_action>
1. Research Gemini Batch API (capabilities, limits, pricing, request format)
2. Plan batch architecture for two-pass extraction (boundaries batch → content batch)
3. Implement batch backfill approach
4. Run full 711-meeting backfill
5. Complete Plan 03 checkpoint and phase verification
</next_action>
