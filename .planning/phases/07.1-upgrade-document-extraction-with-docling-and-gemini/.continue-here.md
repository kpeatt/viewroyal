---
phase: 07.1-upgrade-document-extraction-with-docling-and-gemini
task: 2
total_tasks: 2
status: in_progress
last_updated: 2026-02-17T22:39:59.117Z
---

<current_state>
Phase 07.1 execution: Plans 01 and 02 fully complete. Plan 03 task 1 complete (backfill CLI), task 2 is a human-verify checkpoint — pipeline is functional and tested on 3 meetings, but gemini_extractor.py has uncommitted fixes from debugging session that need to be committed before continuing verification.

All 3 plans executed via wave-based execution. During the Plan 03 checkpoint (end-to-end verification), we discovered and fixed several issues with large PDF handling in gemini_extractor.py. Those fixes are working but the latest round of changes (inline limit bump to 50MB) hasn't been tested in a full pipeline run yet.
</current_state>

<completed_work>

- Plan 07.1-01 (Wave 1): Schema + Gemini extractor — DONE
  - Created `extracted_documents` and `document_images` tables via Supabase migration
  - Added `extracted_document_id` FK to `document_sections`
  - Created `gemini_extractor.py` (438 lines): two-pass extraction with boundary detection + content extraction
  - Commits: bcd42dc9, 4af66f85, 45f9eb07

- Plan 07.1-02 (Wave 2): Image extractor + pipeline integration — DONE
  - Created `image_extractor.py` (PyMuPDF image extraction with R2 upload, dimension filtering)
  - Created `document_extractor.py` (orchestrator: Gemini boundaries -> content -> images -> DB insertion, with fallback to old chunker)
  - Updated `ingester.py` to call new extractor instead of old chunker
  - Updated `orchestrator.py` backfill to use new pipeline
  - Removed Docling from pyproject.toml, added boto3
  - Commits: 3adf2a78, fd9e419a, 52eef13a

- Plan 07.1-03 Task 1: Backfill CLI — DONE
  - Created `backfill_extracted_documents()` method on Archiver class
  - Added `--extract-documents` CLI flag with `--limit` and `--force` support
  - Resumable progress tracking via local JSON file
  - Commit: b7a56c73

- Debugging/fixes (during checkpoint verification):
  - Fixed adaptive PDF splitting for >50MB files (Gemini PDF limit is 50MB, not 2GB)
  - Added page-range extraction for content extraction of large PDFs
  - Commits: 7d5e07c5, a1406b55
</completed_work>

<remaining_work>

- Commit uncommitted changes to gemini_extractor.py:
  - MAX_INLINE_MB raised from 20 to 50 (Google raised inline payload limit to 100MB in Jan 2026)
  - Image downsampling (rewrite_images dpi=72) in chunk creation
  - These changes fix the "Failed to create file" File API rate limit errors seen during batch testing

- Plan 07.1-03 Task 2: Human verification checkpoint
  - Need to re-run `--extract-documents --limit 3 --skip-embed` with the inline fix to confirm no File API errors
  - Then run a larger batch (--limit 10) to validate quality across meeting types
  - Verify resumability works correctly
  - Type "approved" to complete the checkpoint

- Phase verification (automatic after checkpoint approval)
  - GSD verifier checks phase goal achievement
  - Roadmap/STATE.md updates

- Database cleanup: all test extraction data was deleted before pause (extracted_documents and document_sections tables are empty). Progress file also deleted. Fresh start for next session.
</remaining_work>

<decisions_made>

- Gemini PDF processing limit is 50MB (not 2GB) — confirmed via Google docs. The 2GB limit is for general File API storage, not PDF processing.
- Inline data payload limit raised to 100MB (Jan 2026 Google update), so PDFs under 50MB should always use inline bytes to avoid File API rate limits during batch processing.
- Adaptive PDF splitting: for PDFs >50MB, split into chunks targeting 30% of limit (15MB) per chunk based on avg MB/page, then validate actual chunk size and halve if still >50MB. Chunks get image downsampling (72 DPI) and ez_save compression.
- Fallback to old PyMuPDF chunker is kept for cases where Gemini completely fails (returns no boundaries).
- R2 image uploads are optional — pipeline continues gracefully if R2 credentials are not configured.
</decisions_made>

<blockers>
- None — pipeline is functional, just needs the final inline fix tested and checkpoint approved.
</blockers>

<context>
The phase execution is in the Plan 03 human-verify checkpoint. The main code work is done. What remains is:
1. Commit the last gemini_extractor.py changes (inline limit bump)
2. Run a clean test to confirm the File API rate limit issue is resolved
3. Approve the checkpoint so the GSD executor can create the SUMMARY.md
4. Let the phase verifier run

The pipeline successfully processed 3 meetings (55MB, 48.6MB, 48.6MB PDFs) producing 118 boundaries and 326 sections total. The 55MB PDF was handled via adaptive splitting into 4 chunks. The two 48.6MB PDFs hit File API rate limits on some content extractions — the inline fix (MAX_INLINE_MB=50) should eliminate this.

Key files to review on resume:
- apps/pipeline/pipeline/ingestion/gemini_extractor.py (has uncommitted changes)
- apps/pipeline/pipeline/ingestion/document_extractor.py
- apps/pipeline/pipeline/orchestrator.py (backfill_extracted_documents method)
</context>

<next_action>
1. Commit the uncommitted gemini_extractor.py changes (inline limit + image downsampling)
2. Run: `cd apps/pipeline && uv run python main.py --extract-documents --limit 3 --skip-embed`
3. Verify no "Failed to create file" errors and all sections created
4. Resume the Plan 03 checkpoint by spawning a continuation agent with "approved"
5. Let phase verification and roadmap updates complete
</next_action>
