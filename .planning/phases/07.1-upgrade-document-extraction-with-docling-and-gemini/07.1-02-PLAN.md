---
phase: 07.1-upgrade-document-extraction-with-docling-and-gemini
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/pipeline/pipeline/ingestion/docling_extractor.py
  - apps/pipeline/pipeline/ingestion/image_extractor.py
autonomous: true
requirements:
  - DOC-01
  - DOC-03

must_haves:
  truths:
    - "Docling can extract structured markdown content from a specific page range of a PDF"
    - "Docling output is split into heading-based sections with title, text, page range, and order"
    - "Sections exceeding 8000 chars are split at paragraph boundaries"
    - "Sections have tsvector-compatible text for full-text search"
    - "PyMuPDF extracts images from a PDF page range filtering by minimum dimensions"
    - "Extracted images are uploaded to Cloudflare R2 via S3-compatible API"
  artifacts:
    - path: "apps/pipeline/pipeline/ingestion/docling_extractor.py"
      provides: "Docling content extraction with section splitting"
      min_lines: 150
    - path: "apps/pipeline/pipeline/ingestion/image_extractor.py"
      provides: "PyMuPDF image extraction + R2 upload"
      min_lines: 100
  key_links:
    - from: "apps/pipeline/pipeline/ingestion/docling_extractor.py"
      to: "docling.document_converter"
      via: "DocumentConverter.convert(source, page_range)"
      pattern: "converter\\.convert"
    - from: "apps/pipeline/pipeline/ingestion/image_extractor.py"
      to: "Cloudflare R2"
      via: "boto3 S3 client put_object"
      pattern: "put_object"
---

<objective>
Create the Docling content extraction module and the PyMuPDF image extraction + R2 upload module.

Purpose: Docling replaces PyMuPDF font-analysis for heading detection and section splitting with ML-based layout analysis. The image extractor captures meaningful images (maps, charts, diagrams) from PDFs and uploads them to Cloudflare R2 for edge serving. Together with Plan 01's Gemini module, these form the three tools of the new extraction pipeline.

Output: `docling_extractor.py` with section-level extraction from page ranges, `image_extractor.py` with dimension-filtered image extraction and R2 upload.
</objective>

<execution_context>
@/Users/kyle/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kyle/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-CONTEXT.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-RESEARCH.md
@apps/pipeline/pipeline/ingestion/document_chunker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Docling content extraction module</name>
  <files>apps/pipeline/pipeline/ingestion/docling_extractor.py</files>
  <action>
Create `apps/pipeline/pipeline/ingestion/docling_extractor.py` implementing:

**Constants:**
- `MAX_SECTION_CHARS = 8000` (match embed.py MAX_EMBED_CHARS)
- `MIN_SECTION_CHARS = 150` (match existing document_chunker.py)
- `OMP_NUM_THREADS = "4"` (set via os.environ.setdefault at module load)

**Singleton converter initialization:**
```python
_converter: DocumentConverter | None = None

def get_converter() -> DocumentConverter:
    """Get or create the Docling converter (reuses ML models across calls)."""
    global _converter
    if _converter is None:
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = False
        pipeline_options.generate_page_images = False
        pipeline_options.generate_picture_images = False
        _converter = DocumentConverter(
            format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}
        )
        _converter.initialize_pipeline(InputFormat.PDF)
    return _converter
```

**Main function: `extract_sections(pdf_path: str, page_start: int, page_end: int, document_title: str = "") -> list[dict]`**

Logic:
1. Get converter via `get_converter()`
2. Convert the page range: `converter.convert(source=pdf_path, page_range=(page_start, page_end + 1))` — add 1 to page_end to work around Docling's known off-by-one issue (#1469)
3. Export to markdown: `result.document.export_to_markdown()`
4. Split the markdown into sections at heading boundaries (`## ` and `# ` markers)
5. For each section:
   - Extract heading text (first `#`/`##` line) as `section_title`
   - Remaining text as `section_text` (including inline markdown tables)
   - Skip sections shorter than MIN_SECTION_CHARS
   - If section_text exceeds MAX_SECTION_CHARS, split at paragraph boundaries (double newline `\n\n`) with "Part N of M" appended to title
   - Calculate approximate token count: `len(section_text) // 4`
   - Assign `section_order` sequentially starting from 1
   - Estimate page_start/page_end for each section based on position within the markdown (proportional to overall page range)
6. Return list of section dicts with keys: `section_title`, `section_text`, `section_order`, `page_start`, `page_end`, `token_count`

**Helper: `_split_markdown_into_sections(markdown: str) -> list[tuple[str, str]]`**
- Split on heading patterns (`^#{1,3} .+$` at start of line)
- Return list of (heading, body) tuples
- If no headings found, treat entire markdown as one section with document_title as heading

**Helper: `_split_oversized_section(title: str, text: str, max_chars: int) -> list[tuple[str, str]]`**
- Split at paragraph boundaries (`\n\n`)
- Greedily accumulate paragraphs until max_chars would be exceeded
- Return list of (title_with_part_suffix, text_chunk) tuples

**Fallback:** If Docling produces empty output (possible for scanned PDFs), return an empty list. The orchestrator (Plan 03) will decide whether to fall back to the existing `document_chunker.py`.

Log: number of sections extracted, total character count, processing time.
  </action>
  <verify>
Run `cd /Users/kyle/development/viewroyal/apps/pipeline && uv run python -c "from pipeline.ingestion.docling_extractor import extract_sections, get_converter; print('import OK')"` to confirm the module imports. Verify `_split_markdown_into_sections` correctly splits a test markdown string.
  </verify>
  <done>
`docling_extractor.py` exists with `extract_sections()` that: initializes Docling converter once, extracts page ranges with off-by-one workaround, splits markdown at headings into sections, handles oversized sections by paragraph-boundary splitting, skips tiny sections, and returns structured section dicts compatible with `document_sections` table columns.
  </done>
</task>

<task type="auto">
  <name>Task 2: Image extraction and R2 upload module</name>
  <files>apps/pipeline/pipeline/ingestion/image_extractor.py</files>
  <action>
Create `apps/pipeline/pipeline/ingestion/image_extractor.py` implementing:

**Constants:**
- `MIN_WIDTH = 100` (skip images narrower than 100px)
- `MIN_HEIGHT = 100` (skip images shorter than 100px)
- `MIN_AREA = 20_000` (skip images smaller than 20K sq pixels)
- `MAX_ASPECT_RATIO = 5.0` (skip very wide/short images like horizontal rules)
- `R2_BUCKET = "viewroyal-document-images"` (configurable via env var `R2_BUCKET_NAME`)

**R2 client initialization:**
```python
_s3_client = None

def get_r2_client():
    """Get or create boto3 S3 client configured for Cloudflare R2."""
    global _s3_client
    if _s3_client is None:
        import boto3
        _s3_client = boto3.client(
            "s3",
            endpoint_url=os.environ.get("R2_ENDPOINT_URL"),
            aws_access_key_id=os.environ.get("R2_ACCESS_KEY_ID"),
            aws_secret_access_key=os.environ.get("R2_SECRET_ACCESS_KEY"),
            region_name="auto",
        )
    return _s3_client
```

**Main function: `extract_images(pdf_path: str, page_start: int, page_end: int, meeting_id: int, extracted_doc_id: int | None = None) -> list[dict]`**

Logic:
1. Open PDF with `fitz.open(pdf_path)`
2. For each page in range (page_start - 1 to page_end, 0-indexed):
   - Get images via `page.get_images(full=True)`
   - For each image reference:
     - Extract via `doc.extract_image(xref)`
     - Apply dimension filters: width >= MIN_WIDTH, height >= MIN_HEIGHT, area >= MIN_AREA, aspect ratio <= MAX_ASPECT_RATIO
     - Skip duplicate xrefs (same image on multiple pages)
     - Generate a hash of the image bytes for the filename: `hashlib.md5(image_bytes).hexdigest()`
     - Build R2 key: `{meeting_id}/{extracted_doc_id or 'unlinked'}/{image_hash}.{ext}`
     - Upload to R2 if R2 credentials are configured (skip silently if not — allows running without R2 for testing)
     - Build image metadata dict: `page_number`, `description` (empty — filled later from Gemini), `image_type` (empty — filled later from Gemini), `width`, `height`, `format`, `r2_key`, `r2_url`
3. Return list of image metadata dicts

**Helper: `_upload_to_r2(key: str, data: bytes, content_type: str) -> str | None`**
- Upload using `s3_client.put_object(Bucket=R2_BUCKET, Key=key, Body=data, ContentType=content_type)`
- Return the public URL: `{R2_PUBLIC_URL}/{key}` (R2_PUBLIC_URL from env var)
- On failure, log warning and return None (non-fatal — image metadata still tracked)

**Helper: `_content_type_for_ext(ext: str) -> str`**
- Map common image extensions: png -> image/png, jpg/jpeg -> image/jpeg, etc.

**Graceful degradation:** If R2 env vars are not set, log a warning once and skip uploads. Still return image metadata with r2_key and r2_url as None. This allows the pipeline to run locally for testing without R2 configured.

Log: number of images found, number passing filters, number uploaded, total upload size.
  </action>
  <verify>
Run `cd /Users/kyle/development/viewroyal/apps/pipeline && uv run python -c "from pipeline.ingestion.image_extractor import extract_images; print('import OK')"` to confirm the module imports. The module should import even without R2 credentials configured (lazy initialization).
  </verify>
  <done>
`image_extractor.py` exists with `extract_images()` that: opens PDF page ranges via PyMuPDF, filters images by dimension thresholds (100x100 min, 20K area min, 5:1 aspect ratio max), deduplicates by xref, uploads to R2 via boto3 S3-compatible API with graceful degradation when R2 is not configured, and returns image metadata dicts compatible with the `document_images` table.
  </done>
</task>

</tasks>

<verification>
- `docling_extractor.py` imports cleanly
- `image_extractor.py` imports cleanly (even without R2 credentials)
- `extract_sections()` accepts (pdf_path, page_start, page_end, document_title) and returns list[dict]
- `extract_images()` accepts (pdf_path, page_start, page_end, meeting_id) and returns list[dict]
- Section splitting correctly handles headings, oversized sections, and min-size filtering
- Image filtering correctly rejects small, decorative, and extreme-aspect-ratio images
- Docling converter is initialized once (singleton pattern)
- R2 client is lazy-loaded and degrades gracefully when credentials are missing
</verification>

<success_criteria>
Both content extraction modules are complete and independently testable. Docling extracts structured sections from PDF page ranges with heading-based splitting and size constraints. PyMuPDF extracts meaningful images with dimension filtering and uploads to R2. Both modules follow the singleton pattern for expensive initializations (ML models, API clients).
</success_criteria>

<output>
After completion, create `.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-02-SUMMARY.md`
</output>
