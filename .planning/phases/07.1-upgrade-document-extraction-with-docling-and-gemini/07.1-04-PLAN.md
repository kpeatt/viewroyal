---
phase: 07.1-upgrade-document-extraction-with-docling-and-gemini
plan: 04
type: execute
wave: 3
depends_on:
  - "07.1-03"
files_modified:
  - apps/pipeline/pipeline/orchestrator.py
  - apps/pipeline/main.py
autonomous: true
requirements:
  - DOC-02
  - DOC-05

must_haves:
  truths:
    - "Running --backfill-documents processes all 711+ meetings with agenda PDFs from the local archive"
    - "Backfill deletes ALL existing document_sections and extracted_documents before starting"
    - "Backfill tracks progress in a local JSON file and can be interrupted and resumed"
    - "Each meeting's agenda PDF is located via the archive directory structure"
    - "After backfill completes, running --embed-only generates embeddings for all new sections"
    - "Progress reporting shows meeting count, section count, error count, and estimated time remaining"
  artifacts:
    - path: "apps/pipeline/pipeline/orchestrator.py"
      provides: "Updated backfill_document_sections method with resumable processing"
    - path: "apps/pipeline/main.py"
      provides: "Updated CLI with --backfill-documents flag"
  key_links:
    - from: "apps/pipeline/pipeline/orchestrator.py"
      to: "apps/pipeline/pipeline/ingestion/document_extractor.py"
      via: "extract_document() call in backfill loop"
      pattern: "extract_document"
    - from: "apps/pipeline/main.py"
      to: "apps/pipeline/pipeline/orchestrator.py"
      via: "backfill_document_sections() call"
      pattern: "backfill_document_sections"
---

<objective>
Update the backfill pipeline and CLI to process all 711+ meetings with the new Gemini+Docling+PyMuPDF extraction pipeline, with resumable progress tracking.

Purpose: This is the data migration plan. It processes all 722 agenda PDFs (~65K pages) from the local archive using the new three-tool pipeline, replacing all existing document_sections data. The backfill must be resumable since it takes significant time (~10+ minutes for Gemini calls, ~8+ hours for Docling at 0.7s/page) and costs real money (~$6-10 Gemini API).

Output: Updated `orchestrator.py` with resumable backfill, updated `main.py` with `--backfill-documents` flag.
</objective>

<execution_context>
@/Users/kyle/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kyle/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-CONTEXT.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-RESEARCH.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-03-SUMMARY.md
@apps/pipeline/pipeline/orchestrator.py
@apps/pipeline/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Resumable backfill in orchestrator.py</name>
  <files>apps/pipeline/pipeline/orchestrator.py</files>
  <action>
Replace/rewrite the `backfill_document_sections()` method in `apps/pipeline/pipeline/orchestrator.py` (currently lines 435-558) with a new implementation:

**New method signature:**
```python
def backfill_document_sections(self, force=False, resume=True):
```

**Progress tracking:**
- Progress file: `apps/pipeline/backfill_progress.json`
- Structure: `{"processed_meeting_ids": [...], "errors": {"meeting_id": "error_msg"}, "started_at": "iso_timestamp", "last_updated": "iso_timestamp", "total_meetings": N, "total_sections": N, "total_images": N}`
- Load at start, save after each meeting
- If `resume=True` and progress file exists: skip already-processed meeting IDs
- If `force=True`: delete progress file and start fresh

**Implementation logic:**

1. **Delete existing data** (if starting fresh / not resuming from a partial run):
   - If no progress file exists OR force=True:
     - `DELETE FROM document_images WHERE municipality_id = {municipality_id}`
     - `DELETE FROM document_sections WHERE municipality_id = {municipality_id}`
     - `DELETE FROM extracted_documents WHERE municipality_id = {municipality_id}`
     - Print: "Deleted existing document extraction data"

2. **Fetch all meetings with agenda PDFs:**
   - Query: `supabase.table("meetings").select("id, archive_path, date, meeting_type").eq("has_agenda", True).eq("municipality_id", municipality_id).order("date").execute()`
   - Also fetch all documents: `supabase.table("documents").select("id, meeting_id, title, file_path, category").eq("municipality_id", municipality_id).execute()`
   - Group documents by meeting_id

3. **For each meeting (with tqdm progress bar):**
   - Skip if meeting_id in progress.processed_meeting_ids (resume)
   - Find the main agenda PDF: look for documents where file_path contains "Agenda/" or category is "agenda"
   - Resolve PDF path: `{archive_path}/{file_path}`
   - If PDF not found on disk: log error, add to errors, continue
   - Call `extract_document(pdf_path, doc_id, meeting_id, gemini_api_key, supabase, municipality_id)`
   - Insert results into DB (same pattern as ingester.py Task 2 from Plan 03):
     - Insert `extracted_documents` first
     - Insert `document_sections` with `extracted_document_id`
     - Insert `document_images`
   - On success: add meeting_id to progress.processed_meeting_ids, increment counters
   - On error: log error, add to progress.errors, continue (don't stop the batch)
   - Save progress after each meeting
   - Rate limit: `time.sleep(1.0)` between meetings for Gemini API courtesy (adjustable)

4. **Print final summary:**
   - Total meetings processed / total with errors / total skipped (already processed)
   - Total sections created
   - Total images extracted
   - Total time elapsed
   - Estimated Gemini cost (based on page count * 258 tokens/page * $0.10/1M)

5. **Do NOT run embedding generation** â€” that's a separate pass called from main.py after backfill completes (consistent with existing two-pass approach).

**Also add a helper method:**
```python
def _insert_extraction_results(self, result: dict, doc_id: int, meeting_id: int, supabase, municipality_id: int):
    """Insert extract_document results into DB. Shared by backfill and ingester."""
```

This extracts the common insertion logic from ingester.py Task 2 and the backfill, so both use the same code path. Update ingester.py to call this helper too (import from orchestrator or make it a standalone function in document_extractor.py).

Consider placing this helper in `document_extractor.py` as `insert_extraction_results()` so both ingester.py and orchestrator.py can import it.
  </action>
  <verify>
Run `cd /Users/kyle/development/viewroyal/apps/pipeline && uv run python -c "from pipeline.orchestrator import Archiver; a = Archiver(); print(hasattr(a, 'backfill_document_sections'))"` to confirm the method exists. Check that `backfill_progress.json` path is correct.
  </verify>
  <done>
`backfill_document_sections()` method is rewritten with: delete-and-replace strategy for fresh starts, resumable progress tracking via local JSON file, tqdm progress bars, per-meeting error handling (continues on failure), rate limiting between Gemini calls, and comprehensive summary statistics. Insertion logic is shared with the ingester via a common helper function.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update CLI and test single-meeting extraction</name>
  <files>apps/pipeline/main.py</files>
  <action>
1. **Update `apps/pipeline/main.py`:**

   Add new CLI flag:
   ```python
   parser.add_argument(
       "--backfill-documents",
       action="store_true",
       help="Re-extract all documents using Gemini+Docling pipeline. Deletes existing sections and replaces. Resumable.",
   )
   ```

   Add handler after the existing `--backfill-sections` block:
   ```python
   elif args.backfill_documents:
       print("\n--- Backfill Documents (Gemini + Docling + PyMuPDF) ---")
       app.backfill_document_sections(force=args.force)
       if not args.skip_embed:
           print("\n--- Embedding Document Sections ---")
           app._embed_new_content()
   ```

   Keep the existing `--backfill-sections` flag working (it still calls the old method for backward compatibility, though it will be functionally replaced).

2. **Verify end-to-end with a single meeting:**

   After writing the code, run a quick smoke test:
   ```bash
   cd /Users/kyle/development/viewroyal/apps/pipeline
   uv run python main.py --municipality view-royal --target 42 --skip-embed
   ```

   This should:
   - Process meeting ID 42 with the new pipeline
   - Use Gemini to detect document boundaries
   - Use Docling to extract sections
   - Insert extracted_documents and document_sections rows
   - Print summary of what was extracted

   If GEMINI_API_KEY is not set, it should fall back to the legacy chunker and still succeed.

3. **Verify the import chain works:**
   ```bash
   uv run python -c "
   from pipeline.orchestrator import Archiver
   from pipeline.ingestion.document_extractor import extract_document
   from pipeline.ingestion.gemini_boundary import detect_boundaries
   from pipeline.ingestion.docling_extractor import extract_sections
   from pipeline.ingestion.image_extractor import extract_images
   print('All imports successful')
   "
   ```
  </action>
  <verify>
Run `cd /Users/kyle/development/viewroyal/apps/pipeline && uv run python main.py --help | grep backfill-documents` to confirm the new flag appears. Run the import chain verification. If GEMINI_API_KEY is available in the environment, run the single-meeting smoke test.
  </verify>
  <done>
CLI has `--backfill-documents` flag for the full backfill. The entire import chain works (main.py -> orchestrator -> document_extractor -> gemini_boundary/docling_extractor/image_extractor). Single-meeting extraction works via `--target`. The pipeline gracefully falls back to the legacy chunker when Gemini is unavailable.
  </done>
</task>

</tasks>

<verification>
- `--backfill-documents` flag appears in CLI help
- `backfill_document_sections()` handles: fresh start (deletes existing), resume (skips processed), force (re-does everything)
- Progress file is created/updated at `apps/pipeline/backfill_progress.json`
- tqdm progress bar shows during processing
- Errors on individual meetings don't stop the batch
- Rate limiting between Gemini calls (1s minimum)
- All imports in the chain work cleanly
- Single-meeting extraction works via `--target`
- Summary statistics printed at end (meetings, sections, images, estimated cost)
</verification>

<success_criteria>
The full backfill pipeline is operational. Running `--backfill-documents` processes all 711+ meetings, can be interrupted and resumed, tracks progress locally, and reports comprehensive statistics. Running `--target N` processes a single meeting with the new pipeline. The existing `--backfill-sections` still works as a legacy fallback. After backfill, `--embed-only` generates embeddings for all new sections.
</success_criteria>

<output>
After completion, create `.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-04-SUMMARY.md`
</output>
