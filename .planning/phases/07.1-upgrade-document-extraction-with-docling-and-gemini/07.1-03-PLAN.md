---
phase: 07.1-upgrade-document-extraction-with-docling-and-gemini
plan: 03
type: execute
wave: 2
depends_on:
  - "07.1-01"
  - "07.1-02"
files_modified:
  - apps/pipeline/pipeline/ingestion/document_extractor.py
  - apps/pipeline/pipeline/ingestion/ingester.py
autonomous: true
requirements:
  - DOC-01
  - DOC-02
  - DOC-04

must_haves:
  truths:
    - "The orchestrator module calls Gemini, Docling, and PyMuPDF in sequence for a given PDF"
    - "For each Gemini-detected document boundary, Docling extracts sections and PyMuPDF extracts images"
    - "Sections are stored in document_sections with extracted_document_id populated"
    - "Extracted document metadata is stored in extracted_documents table"
    - "Image metadata is stored in document_images table"
    - "Agenda item linking uses Gemini-provided agenda_item mapping instead of heuristic matching"
    - "When Gemini/Docling fail, the pipeline falls back to the existing document_chunker.py"
    - "ingester.py _ingest_document_sections calls the new extractor for agenda PDFs"
  artifacts:
    - path: "apps/pipeline/pipeline/ingestion/document_extractor.py"
      provides: "Three-tool pipeline orchestrator"
      min_lines: 200
    - path: "apps/pipeline/pipeline/ingestion/ingester.py"
      provides: "Modified _ingest_document_sections method"
  key_links:
    - from: "apps/pipeline/pipeline/ingestion/document_extractor.py"
      to: "apps/pipeline/pipeline/ingestion/gemini_boundary.py"
      via: "detect_boundaries() call"
      pattern: "detect_boundaries"
    - from: "apps/pipeline/pipeline/ingestion/document_extractor.py"
      to: "apps/pipeline/pipeline/ingestion/docling_extractor.py"
      via: "extract_sections() call"
      pattern: "extract_sections"
    - from: "apps/pipeline/pipeline/ingestion/document_extractor.py"
      to: "apps/pipeline/pipeline/ingestion/image_extractor.py"
      via: "extract_images() call"
      pattern: "extract_images"
    - from: "apps/pipeline/pipeline/ingestion/ingester.py"
      to: "apps/pipeline/pipeline/ingestion/document_extractor.py"
      via: "extract_and_ingest_document() call"
      pattern: "extract_and_ingest_document"
---

<objective>
Create the document extraction orchestrator that ties together Gemini boundary detection, Docling content extraction, and PyMuPDF image extraction. Integrate it into the existing ingester.py pipeline.

Purpose: This plan connects the three extraction tools (Plans 01+02) into a coherent pipeline and wires it into the existing ingestion flow. When the pipeline processes a meeting with an agenda PDF, it now uses Gemini to identify document boundaries, Docling to extract content, and PyMuPDF to extract images — replacing the old PyMuPDF font-analysis approach.

Output: `document_extractor.py` orchestrator module, modified `ingester.py` with new extraction flow.
</objective>

<execution_context>
@/Users/kyle/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kyle/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-CONTEXT.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-RESEARCH.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-01-SUMMARY.md
@.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-02-SUMMARY.md
@apps/pipeline/pipeline/ingestion/ingester.py
@apps/pipeline/pipeline/ingestion/document_chunker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Document extraction orchestrator module</name>
  <files>apps/pipeline/pipeline/ingestion/document_extractor.py</files>
  <action>
Create `apps/pipeline/pipeline/ingestion/document_extractor.py` implementing:

**Imports:**
```python
from pipeline.ingestion.gemini_boundary import detect_boundaries
from pipeline.ingestion.docling_extractor import extract_sections
from pipeline.ingestion.image_extractor import extract_images
from pipeline.ingestion.document_chunker import chunk_document  # fallback
```

**Main function: `extract_document(pdf_path: str, document_id: int, meeting_id: int, gemini_api_key: str, supabase, municipality_id: int = 1, agenda_items: list[dict] | None = None) -> dict`**

This is the single entry point for the entire three-tool pipeline. Returns a dict with:
- `extracted_documents`: list of dicts for the `extracted_documents` table
- `sections`: list of dicts for the `document_sections` table
- `images`: list of dicts for the `document_images` table

Logic:
1. **Gemini boundary detection:**
   - Call `detect_boundaries(pdf_path, gemini_api_key)`
   - If Gemini fails (exception, empty result, API error): log warning, fall back to legacy chunker
   - If Gemini returns boundaries: proceed with Docling + PyMuPDF for each boundary

2. **For each Gemini boundary document:**
   a. Create an `extracted_document` dict: `document_id`, `title`, `document_type`, `page_start`, `page_end`, `summary`, `key_facts` (as JSON), `agenda_item_id` (resolved from Gemini's `agenda_item` string), `municipality_id`
   b. **Resolve agenda_item_id:** Take Gemini's `agenda_item` string (e.g., "6.1a", "5a") and match it against the provided `agenda_items` list. Match by comparing the item_number field. If no match, try fuzzy title matching against the document title. If still no match, leave as None.
   c. **Docling extraction:** Call `extract_sections(pdf_path, boundary.page_start, boundary.page_end, boundary.title)`
      - If Docling returns empty (scanned PDF or extraction failure): log warning, use `chunk_document(pdf_path, boundary.title)` as fallback limited to the page range
   d. **Image extraction:** Call `extract_images(pdf_path, boundary.page_start, boundary.page_end, meeting_id)`
      - Enrich image metadata with Gemini's image descriptions: match by page number to Gemini's `images` array, fill in `description` and `image_type`
   e. Assign `section_order` sequentially across all documents (not resetting per document)

3. **Fallback path (when Gemini fails entirely):**
   - Call `chunk_document(pdf_path, doc_title)` from existing `document_chunker.py`
   - Call `link_sections_to_agenda_items(sections, meeting_id, supabase)` from existing chunker
   - Return sections without extracted_documents or images
   - This ensures the pipeline never completely fails — worst case it behaves like the old pipeline

4. Return structured result dict

**Helper: `_resolve_agenda_item_id(agenda_item_str: str | None, agenda_items: list[dict]) -> int | None`**
- If agenda_item_str is None, return None
- Normalize the string: strip whitespace, lowercase
- Match against agenda_items by `item_number` field (case-insensitive, whitespace-normalized)
- If exact match found, return its `id`
- If no match, try fuzzy: check if the string appears as a substring in any item_number
- Return None if no match

**Helper: `_fetch_agenda_items_for_meeting(meeting_id: int, supabase) -> list[dict]`**
- Query: `supabase.table("agenda_items").select("id, item_number, title").eq("meeting_id", meeting_id).execute()`
- Return the data list
- Cache per meeting_id (simple dict cache) to avoid repeated queries during backfill
  </action>
  <verify>
Run `cd /Users/kyle/development/viewroyal/apps/pipeline && uv run python -c "from pipeline.ingestion.document_extractor import extract_document; print('import OK')"` to confirm the module imports correctly. Verify fallback import from document_chunker works.
  </verify>
  <done>
`document_extractor.py` exists with `extract_document()` that: calls Gemini for boundary detection, iterates boundaries calling Docling for sections and PyMuPDF for images, resolves agenda items from Gemini's string mappings, enriches images with Gemini descriptions, and falls back to the legacy chunker when Gemini/Docling fail. Returns structured data ready for database insertion.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate new extractor into ingester.py</name>
  <files>apps/pipeline/pipeline/ingestion/ingester.py</files>
  <action>
Modify `apps/pipeline/pipeline/ingestion/ingester.py` to use the new document extraction pipeline:

1. **Add import** at top of file (with other ingestion imports):
   ```python
   from pipeline.ingestion.document_extractor import extract_document
   ```

2. **Replace `_ingest_document_sections()` method** (lines 769-855) with a new implementation that:

   a. Fetches documents for the meeting (same as current)
   b. For each document with a file_path:
      - Check idempotency: skip if document already has sections (unless force=True). Same logic as current.
      - Resolve full PDF path (same as current)
      - Check if this is an agenda package PDF: look for "Agenda" in the file_path or document category. This determines whether to use the new Gemini+Docling pipeline or the legacy chunker.
      - **If agenda PDF (new pipeline):**
        - Get GEMINI_API_KEY from `pipeline.config` (add `GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")` to config if not already there)
        - If no GEMINI_API_KEY, fall back to legacy chunker
        - Call `extract_document(pdf_path, doc_id, meeting_id, gemini_api_key, self.supabase, self.municipality_id)`
        - Insert `extracted_documents` rows first, get back their IDs
        - Insert `document_sections` rows with `extracted_document_id` populated
        - Insert `document_images` rows
      - **If non-agenda PDF (legacy path):**
        - Call `chunk_document(pdf_path, doc_title)` (unchanged)
        - Call `link_sections_to_agenda_items(sections, meeting_id, self.supabase)` (unchanged)
        - Insert `document_sections` rows (same as current, no extracted_document_id)

   c. **Insertion logic for new pipeline results:**
      ```python
      result = extract_document(pdf_path, doc_id, meeting_id, gemini_api_key, self.supabase, self.municipality_id)

      # Insert extracted_documents first (need IDs for sections and images)
      ed_id_map = {}  # index -> db id
      for i, ed in enumerate(result.get("extracted_documents", [])):
          ed["document_id"] = doc_id
          ed["municipality_id"] = self.municipality_id
          resp = self.supabase.table("extracted_documents").insert(ed).execute()
          ed_id_map[i] = resp.data[0]["id"]

      # Insert sections with extracted_document_id
      section_rows = []
      for s in result.get("sections", []):
          row = {
              "document_id": doc_id,
              "extracted_document_id": s.get("extracted_document_id"),
              "agenda_item_id": s.get("agenda_item_id"),
              "section_title": s.get("section_title"),
              "section_text": s["section_text"],
              "section_order": s["section_order"],
              "page_start": s.get("page_start"),
              "page_end": s.get("page_end"),
              "token_count": s.get("token_count"),
              "municipality_id": self.municipality_id,
          }
          section_rows.append(row)

      # Batch insert sections
      batch_size = 50
      for i in range(0, len(section_rows), batch_size):
          batch = section_rows[i : i + batch_size]
          self.supabase.table("document_sections").insert(batch).execute()

      # Insert image metadata
      for img in result.get("images", []):
          img["document_id"] = doc_id
          img["municipality_id"] = self.municipality_id
      if result.get("images"):
          self.supabase.table("document_images").insert(result["images"]).execute()
      ```

   d. Print summary: number of extracted documents, sections, and images per PDF.

3. **Add GEMINI_API_KEY to pipeline/config.py** if not already present:
   ```python
   GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
   ```

4. **Keep the existing `_ingest_document_sections` call site unchanged** — the method signature stays the same: `_ingest_document_sections(self, meeting_id, folder_path)`. The new/old pipeline selection happens internally.
  </action>
  <verify>
Run `cd /Users/kyle/development/viewroyal/apps/pipeline && uv run python -c "from pipeline.ingestion.ingester import MeetingIngester; print('import OK')"` to confirm the modified ingester imports. Verify the method signature of `_ingest_document_sections` is unchanged.
  </verify>
  <done>
`ingester.py` routes agenda PDFs through the new Gemini+Docling+PyMuPDF pipeline while routing non-agenda PDFs through the legacy chunker. Extracted documents, sections, and images are all inserted into their respective tables. Fallback to legacy chunker works when GEMINI_API_KEY is missing or when Gemini fails. The external API of `_ingest_document_sections()` is unchanged.
  </done>
</task>

</tasks>

<verification>
- `document_extractor.py` imports cleanly and calls all three sub-modules
- `ingester.py` imports cleanly with the new extractor
- `_ingest_document_sections` method signature is unchanged
- Agenda PDFs route to new pipeline (Gemini+Docling+PyMuPDF)
- Non-agenda PDFs route to legacy chunker
- Fallback works when GEMINI_API_KEY is missing
- `extracted_documents` rows are inserted before `document_sections` (FK dependency)
- `document_images` rows are inserted with correct `extracted_document_id`
- `GEMINI_API_KEY` is in pipeline config
</verification>

<success_criteria>
The three extraction tools are connected through a single orchestrator. The ingester selects the appropriate pipeline (new vs legacy) based on document type. Database insertions respect FK ordering. Fallback to the legacy chunker ensures the pipeline never completely fails. A single meeting can be processed end-to-end with `--target` flag.
</success_criteria>

<output>
After completion, create `.planning/phases/07.1-upgrade-document-extraction-with-docling-and-gemini/07.1-03-SUMMARY.md`
</output>
