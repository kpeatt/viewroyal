---
phase: 07-document-intelligence
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/pipeline/pipeline/ingestion/document_chunker.py
  - apps/pipeline/pipeline/ingestion/ingester.py
  - apps/pipeline/pipeline/ingestion/embed.py
autonomous: true
requirements:
  - DOC-01
  - DOC-02
  - DOC-03
  - DOC-04

must_haves:
  truths:
    - "Running the pipeline on a meeting with PDF attachments produces document_sections rows with heading-derived titles and section text"
    - "Each document section has a halfvec(384) embedding column and a populated tsvector full-text search column"
    - "Document sections are linked to agenda items via agenda_item_id when number-pattern or title matching succeeds"
    - "Headingless PDFs produce sections named '{doc_title} - Section N of M' using fixed-size paragraph-boundary splitting"
    - "Oversized sections (>8000 chars) are split at paragraph boundaries into sub-sections"
  artifacts:
    - path: "apps/pipeline/pipeline/ingestion/document_chunker.py"
      provides: "PDF heading detection, chunking, agenda-item linking"
      min_lines: 100
    - path: "apps/pipeline/pipeline/ingestion/ingester.py"
      provides: "_ingest_document_sections() call after _ingest_documents()"
      contains: "_ingest_document_sections"
    - path: "apps/pipeline/pipeline/ingestion/embed.py"
      provides: "document_sections entry in TABLE_CONFIG"
      contains: "document_sections"
  key_links:
    - from: "apps/pipeline/pipeline/ingestion/ingester.py"
      to: "apps/pipeline/pipeline/ingestion/document_chunker.py"
      via: "import and call chunk_document()"
      pattern: "chunk_document"
    - from: "apps/pipeline/pipeline/ingestion/embed.py"
      to: "document_sections table"
      via: "TABLE_CONFIG entry"
      pattern: "document_sections"
    - from: "document_chunker.py"
      to: "agenda_items table"
      via: "number-pattern matching within same meeting"
      pattern: "agenda_item_id"
---

<objective>
Create the `document_sections` database table and the pipeline chunking module that splits PDF documents into searchable, embeddable sections linked to agenda items.

Purpose: Every PDF in the system needs to be broken into section-level chunks for granular search and embedding. This plan creates the database schema and the Python chunker that produces those sections during pipeline ingestion.

Output: Migration creating `document_sections` table with indexes + `match_document_sections` RPC. New `document_chunker.py` module. Integration into `ingester.py` and `embed.py`.
</objective>

<execution_context>
@/Users/kyle/.claude/get-shit-done/workflows/execute-plan.md
@/Users/kyle/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-document-intelligence/07-RESEARCH.md
@.planning/phases/07-document-intelligence/07-CONTEXT.md
@apps/pipeline/pipeline/ingestion/ingester.py
@apps/pipeline/pipeline/ingestion/embed.py
@apps/pipeline/pipeline/marker_parser.py
@apps/pipeline/pipeline/parser.py
@sql/bootstrap.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create document_sections table with migration</name>
  <files>Supabase migration (via apply_migration MCP tool)</files>
  <action>
Apply a Supabase migration named `create_document_sections` with the following SQL:

```sql
-- Table
CREATE TABLE document_sections (
    id bigint generated by default as identity primary key,
    document_id bigint REFERENCES documents(id) ON DELETE CASCADE NOT NULL,
    agenda_item_id bigint REFERENCES agenda_items(id) ON DELETE SET NULL,
    section_title text,
    section_text text NOT NULL,
    section_order integer NOT NULL,
    page_start integer,
    page_end integer,
    token_count integer,
    embedding halfvec(384),
    text_search tsvector GENERATED ALWAYS AS (
        to_tsvector('english', coalesce(section_title, '') || ' ' || coalesce(section_text, ''))
    ) STORED,
    municipality_id bigint REFERENCES municipalities(id) DEFAULT 1,
    created_at timestamptz DEFAULT timezone('utc'::text, now()) NOT NULL
);

-- Indexes
CREATE INDEX idx_document_sections_document ON document_sections(document_id);
CREATE INDEX idx_document_sections_agenda_item ON document_sections(agenda_item_id);
CREATE INDEX idx_document_sections_search ON document_sections USING gin(text_search);
CREATE INDEX idx_document_sections_embedding ON document_sections
    USING hnsw (embedding halfvec_cosine_ops);

-- RLS
ALTER TABLE document_sections ENABLE ROW LEVEL SECURITY;
CREATE POLICY "Allow public read access" ON document_sections FOR SELECT USING (true);

-- Match function for vector similarity search
CREATE OR REPLACE FUNCTION match_document_sections (
  query_embedding halfvec(384),
  match_threshold float,
  match_count int,
  filter_municipality_id bigint DEFAULT NULL
)
RETURNS TABLE (
  id bigint,
  document_id bigint,
  agenda_item_id bigint,
  section_title text,
  section_text text,
  section_order int,
  similarity float
)
LANGUAGE plpgsql
SET search_path = 'public'
AS $$
BEGIN
  RETURN QUERY
  SELECT
    ds.id,
    ds.document_id,
    ds.agenda_item_id,
    ds.section_title,
    ds.section_text,
    ds.section_order,
    1 - (ds.embedding <=> query_embedding) AS similarity
  FROM document_sections ds
  WHERE (filter_municipality_id IS NULL OR ds.municipality_id = filter_municipality_id)
    AND 1 - (ds.embedding <=> query_embedding) > match_threshold
  ORDER BY ds.embedding <=> query_embedding
  LIMIT match_count;
END;
$$;
```

This follows the exact pattern from `bylaw_chunks` table (bootstrap.sql lines 369-376) and `match_bylaw_chunks` RPC (bootstrap.sql lines 622-652). The `SET search_path = 'public'` is required to pass Supabase security advisor (per Phase 3 decision).
  </action>
  <verify>Run `SELECT count(*) FROM document_sections;` via execute_sql MCP tool — should return 0 with no errors. Also verify the text_search column exists by running `SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'document_sections' ORDER BY ordinal_position;`</verify>
  <done>document_sections table exists with all columns (including tsvector GENERATED column), indexes (GIN for text_search, HNSW for embedding), RLS policy, and match_document_sections RPC function.</done>
</task>

<task type="auto">
  <name>Task 2: Create document chunker module and integrate into pipeline</name>
  <files>
    apps/pipeline/pipeline/ingestion/document_chunker.py
    apps/pipeline/pipeline/ingestion/ingester.py
    apps/pipeline/pipeline/ingestion/embed.py
  </files>
  <action>
**Create `apps/pipeline/pipeline/ingestion/document_chunker.py`:**

A new module that chunks PDF documents into sections. Core functions:

1. `chunk_document(pdf_path: str, doc_title: str) -> list[dict]` — Main entry point. Returns list of `{section_title, section_text, section_order, page_start, page_end, token_count}`.
   - Opens PDF with PyMuPDF (`fitz`)
   - Calls `_detect_body_font_size()` to find the most-frequent font size by character count
   - If body_size is None or no text blocks (scanned PDF), falls back to `_fixed_size_fallback()`
   - Otherwise calls `_split_at_headings()` to walk all pages and split at heading boundaries
   - If `_split_at_headings()` returns empty (no headings detected), falls back to `_fixed_size_fallback()`
   - Enforces size cap: sections >8000 chars (`MAX_SECTION_CHARS`) are split at paragraph boundaries via `_split_oversized_section()`
   - Filters out trivially small sections (<100 chars, `MIN_SECTION_CHARS`)
   - Re-numbers `section_order` sequentially from 1
   - Computes `token_count` as `int(len(text.split()) * 1.3)` (rough estimate, per research recommendation)

2. `_detect_body_font_size(doc) -> float | None` — Analyze font sizes across all pages using PyMuPDF dict mode. Count characters per font_id (font_name + rounded size). Body font = most frequent by character count. Return its size, or None if no text blocks found.

3. `_split_at_headings(doc, body_size: float) -> list[dict]` — Walk pages sequentially. For each text span:
   - Determine if it's a heading: font size > body_size * 1.2, OR (bold flag set AND (all-caps with len>3 OR size >= body_size))
   - Bold detection: `span["flags"] & (1 << 4)` (bit 4 of PyMuPDF flags)
   - When a heading is detected: finalize the current section (if any text accumulated), start a new section with this heading as title
   - Merge consecutive heading-sized spans on same/adjacent lines into a single heading title (prevents fragmented titles like "STAFF" + "REPORT" becoming two sections). Adjacent = y-positions within 5 points of each other.
   - Track page_start and page_end for each section
   - Use `_clean_text()` from `pipeline.parser` for text cleanup (existing function handles View Royal Unicode issues)

4. `_fixed_size_fallback(pdf_path: str, doc_title: str) -> list[dict]` — For headingless/image PDFs:
   - Extract text via `parser.get_pdf_text()`, with OCR fallback via `parser.get_pdf_text_ocr()` if <100 chars
   - If text is too short (<MIN_SECTION_CHARS), return empty list
   - Split at paragraph boundaries (double newline) into chunks of ~MAX_SECTION_CHARS
   - Title each section: `"{doc_title} - Section {i} of {total}"` (per locked user decision)
   - page_start/page_end = None for fallback sections

5. `_split_oversized_section(section: dict, max_chars: int) -> list[dict]` — Split a section exceeding max_chars at paragraph boundaries (double newlines). If no paragraph boundaries, split at single newlines. Label sub-sections: `"{original_title} - Part {i} of {total}"`.

6. `link_sections_to_agenda_items(sections: list[dict], meeting_id: int, supabase) -> list[dict]` — For each section, attempt to link to an agenda item:
   - Fetch agenda items for this meeting_id from Supabase
   - Strategy 1 (number matching): Extract leading number pattern from section_title (e.g., "8.1" from "8.1 Staff Report"). Match against agenda_item.item_order within the same meeting.
   - Strategy 2 (title matching): If no number match, compare section_title against agenda_item.title using case-insensitive containment. Require title length > 10 chars to avoid false positives on short generic titles.
   - Set `agenda_item_id` on the section dict if a match is found, otherwise leave None.
   - All matching is scoped to same meeting (document.meeting_id = agenda_item.meeting_id) per research Pitfall 4.

**Modify `apps/pipeline/pipeline/ingestion/ingester.py`:**

Add a `_ingest_document_sections()` method to `MeetingIngester` class:
- Called AFTER `_ingest_documents()` in `process_meeting()` method
- Query the `documents` table for all documents belonging to this meeting_id
- For each document with a non-null `file_path`:
  - Resolve the full PDF path from the meeting's folder_path + document.file_path
  - Check if document already has sections (query `document_sections` where document_id = doc.id, count > 0). If yes, skip (idempotency).
  - Call `chunk_document(pdf_path, doc.title)` — wrapped in try/except, log errors and continue (per locked user decision: parse errors log and skip)
  - Call `link_sections_to_agenda_items(sections, meeting_id, self.supabase)`
  - Insert sections into `document_sections` table via Supabase, setting `document_id`, `municipality_id`
  - Log: `"  [+] Chunked {doc.title}: {len(sections)} sections"`

In `process_meeting()`, after the existing `self._ingest_documents(meeting_id, folder_path, dry_run)` call (around line 766), add:
```python
if not dry_run:
    self._ingest_document_sections(meeting_id, folder_path)
```

**Modify `apps/pipeline/pipeline/ingestion/embed.py`:**

Add `document_sections` to `TABLE_CONFIG`:
```python
"document_sections": {
    "select": "id, section_title, section_text",
    "text_fn": lambda r: f"{r[1] or ''}\n{r[2] or ''}".strip(),
},
```

Also add to `DEFAULT_MIN_WORDS`:
```python
"document_sections": 5,
```

This follows the exact pattern of `bylaw_chunks` in the existing TABLE_CONFIG.
  </action>
  <verify>
1. Verify the new file exists: `ls apps/pipeline/pipeline/ingestion/document_chunker.py`
2. Verify embed.py has the new entry: `grep "document_sections" apps/pipeline/pipeline/ingestion/embed.py`
3. Verify ingester.py calls the new method: `grep "_ingest_document_sections" apps/pipeline/pipeline/ingestion/ingester.py`
4. Run `cd apps/pipeline && uv run python -c "from pipeline.ingestion.document_chunker import chunk_document, link_sections_to_agenda_items; print('OK')"` to verify import
  </verify>
  <done>document_chunker.py exists with chunk_document() and link_sections_to_agenda_items() functions. ingester.py calls _ingest_document_sections() after document ingestion. embed.py includes document_sections in TABLE_CONFIG. Pipeline processes PDFs into sections with heading-based or fallback chunking during normal ingestion flow.</done>
</task>

</tasks>

<verification>
1. The `document_sections` table exists in Supabase with all required columns
2. The `text_search` tsvector column is auto-populated on insert
3. The `match_document_sections` RPC function exists and accepts halfvec(384)
4. `document_chunker.py` module imports cleanly
5. Pipeline ingestion flow calls document sectioning after document ingestion
6. `embed.py` TABLE_CONFIG includes document_sections for embedding generation
</verification>

<success_criteria>
- document_sections table has correct schema with GIN and HNSW indexes
- Running pipeline on a meeting with PDFs creates document_sections rows
- Sections have heading-derived titles (or fallback positional titles)
- Sections are linked to agenda items via number/title matching when possible
- embed.py can generate embeddings for document_sections rows
</success_criteria>

<output>
After completion, create `.planning/phases/07-document-intelligence/07-01-SUMMARY.md`
</output>
